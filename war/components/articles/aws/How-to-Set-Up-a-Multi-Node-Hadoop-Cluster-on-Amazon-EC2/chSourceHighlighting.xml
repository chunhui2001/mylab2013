<?xml version="1.0" encoding="utf-8"?>
<c:component type="chSourceHighlighting" componentId="chSourceHighlighting_1" xmlns:c="http://com.snnmo.website">
  <c:abstract>
    <![CDATA[After spending some time playing around on Single-Node pseudo-distributed cluster, it's time to get into real world hadoop.
    Depending on what works best – Its important to note that there are multiple ways to achieve this and I am going to cover how to setup multi-node hadoop cluster on Amazon EC2. 
    We are going to setup 4 node hadoop cluster as below.
    <br />
    <br />
    <ul style="padding-left:1.5em;list-style:disc;">
        <li>NameNode (Master)</li>
        <li>SecondaryNameNode</li>
        <li>DataNode (Slave1)</li>
        <li>DataNode (Slave2)</li>
    </ul>]]>
  </c:abstract>
  

  <c:entry style="margin-top:2em;">
    <c:title>Requirements</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="" id="sourceContent1"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <ul style="padding-left:1.5em;list-style:disc;">
          <li> Amazon AWS Account</li>
          <li> PuTTy Windows Client (to connect to Amazon EC2 instance)</li>
          <li> PuTTYgen (to generate private key – this will be used in putty to connect to EC2 instance)</li>
          <li> WinSCP (secury copy)</li>
      </ul>
    ]]>
    </c:sourceContent>
    


    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.5em;">
    <c:title>Setting up Amazon EC2 Instances</c:title>
    <c:desc>
      <c:desc1> <![CDATA[With 4 node clusters and minimum volume size of 8GB there would be an average $2 of charge per day with all 4 running instances. 
      You can stop the instance anytime to avoid the charge, but you will loose the public IP and host and restarting the instance will create new ones. 
      You can also terminate your Amazon EC2 instance anytime and by default it will delete your instance upon termination, so just be careful what you are doing.]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="Get Amazon AWS Account" id="sourceContent1"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          If you do not already have a account, please create a new one. I already have AWS account and going to skip the sign-up process. Amazon EC2 comes with eligible free-tier instances.
      </span>
      <img src="//c2.staticflickr.com/6/5585/14636640858_c63fc59fba.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>
    
    <c:sourceContent type="html" title="Launch Instance" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          Once you have signed up for Amazon account. Login to Amazon Web Services, click on My Account and navigate to Amazon EC2 Console
      </span>
      <img src="//c1.staticflickr.com/3/2908/14821028494_736c8cc3f7_z.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Select AMI" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          I am picking Ubuntu Server 14.04 LTS 64-bit OS
      </span>
      <img src="//c2.staticflickr.com/4/3884/14636773318_19249991ed_z.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Select Instance Type" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          Select the micro instance
      </span>
      <img src="//c2.staticflickr.com/4/3848/14858092935_d75b5f62e6_b.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Configure Number of Instances" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          As mentioned we are setting up 4 node hadoop cluster, so please enter 4 as number of instances. 
          Please check Amazon EC2 free-tier requirements, you may setup 3 node cluster with < 30GB storage size to avoid any charges.  
          In production environment you want to have SecondayNameNode as separate machine
      </span>
      <img src="//c2.staticflickr.com/4/3859/14878009273_56b330663e_b.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Add Storage" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          Minimum volume size is 8GB
      </span>
      <img src="//c2.staticflickr.com/4/3863/14671511289_04384ea419_b.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Instance Description" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
         Give your instance name and description
      </span>
      <img src="//c2.staticflickr.com/4/3892/14671516679_3dd3286d1c_b.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Define a Security Group" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
         Create a new security group, later on we are going to modify the security group with security rules.
      </span>
      <img src="//c2.staticflickr.com/4/3911/14857807722_0a877e6063_b.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Launch Instance and Create Security Pair" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
         Review and Launch Instance. <br />
         <br />
         Amazon EC2 uses public–key cryptography to encrypt and decrypt login information. 
         Public–key cryptography uses a public key to encrypt a piece of data, such as a password, then the recipient uses the private key to decrypt the data. 
         The public and private keys are known as a <b>key pair</b>. <br />
         <br />
         Create a new keypair and give it a name “hadoopec2cluster” and download the keypair (.pem) file to your local machine. Click Launch Instance
      </span>
      <img src="//c2.staticflickr.com/6/5596/14878037423_a5a6af75da.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Launching Instances" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
         Once you click “Launch Instance” 4 instance should be launched with “pending” state
      </span>
      <img src="//c2.staticflickr.com/6/5593/14671551648_33c5a254c4_b.jpg" style="max-width:100%;" />
      <br />
      <span style="display:block;margin-bottom:.5em;">
         Once in “running” state we are now going to rename the instance name as below.
         
      </span>
      <ul style="padding-left:1.5em;list-style:disc;">
          <li>HadoopNameNode (Master)</li>
          <li>HadoopSecondaryNameNode</li>
          <li>HadoopSlave1 (data node will reside here)</li>
          <li>HaddopSlave2  (data node will reside here)</li>
      </ul>
      <br />
      <img src="//c2.staticflickr.com/6/5596/14671488210_59d4c2f388_b.jpg" style="max-width:100%;" />
      <br />
      <span style="display:block;margin-bottom:.5em;">
         Please note down the Instance ID, Public DNS/URL (ec2-54-209-221-112.compute-1.amazonaws.com)  and Public IP for each instance for your reference.
         We will need it later on to connect from Putty client.  Also notice we are using “HadoopEC2SecurityGroup”.
      </span>
      <img src="//c2.staticflickr.com/6/5584/14671642597_a4397d0af4_b.jpg" style="max-width:100%;" />
      <br />
      <span style="display:block;margin-bottom:.5em;">
        You can use the existing group or create a new one. 
        When you create a group with default options it add a rule for SSH at port 22.In order to have TCP and ICMP access we need to add 2 additional security rules.
        Add ‘All TCP’, ‘All ICMP’ and ‘SSH (22)’ under the inbound rules to “HadoopEC2SecurityGroup”. 
        This will allow ping, SSH, and other similar commands among servers and from any other machine on internet. 
        Make sure to “Apply Rule changes” to save your changes.
        <br />
        <br />
        These protocols and ports are also required to enable communication among cluster servers. 
        As this is a test setup we are allowing access to all for TCP, ICMP and SSH and not bothering about the details of individual server port and security.
      </span>
      <img src="//c2.staticflickr.com/6/5557/14855120971_962b579de5.jpg" style="max-width:100%;" />
    ]]>
    </c:sourceContent>




    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.5em;">
    <c:title>Setting up client access to Amazon Instances</c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[
          Now, lets make sure we can connect to all 4 instances.
          For that we are going to use Putty client We are going setup password-less SSH access among servers to setup the cluster. 
          This allows remote access from Master Server to Slave Servers so Master Server can remotely start the Data Node and Task Tracker services on Slave servers.
            <br /><br />
          We are going to use downloaded hadoopec2cluster.pem file to generate the private key (.ppk). 
          In order to generate the private key we need Puttygen client. 
          You can download the putty and puttygen and various utilities in zip from <a target="_blank" href="http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html">here</a>.
        ]]>
      </c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="Generating Private Key" id="sourceContent1"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          Let’s launch PUTTYGEN client and import the key pair we created during launch instance step – “hadoopec2cluster.pem”
          <br /><br />
          Navigate to Conversions and “Import Key”
      </span>
      <img src="//c2.staticflickr.com/4/3875/14855172271_b7ed4ab7db.jpg" style="max-width:100%;" />
      <br />
      <img src="//c2.staticflickr.com/4/3902/14671617149_24d3aee25b_z.jpg" style="max-width:100%;" />
      <br />
      you import the key You can enter passphrase to protect your private key or leave the passphrase fields blank to use the private key without any passphrase.
      Passphrase protects the private key from any unauthorized access to servers using your machine and your private key.
      <br /><br />
      Any access to server using passphrase protected private key will require the user to enter the passphrase to enable the private key enabled access to AWS EC2 server.
    ]]>
    </c:sourceContent>
    
    <c:sourceContent type="html" title="Save Private Key" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          Now save the private key by clicking on “Save Private Key” and click “Yes” as we are going to leave passphrase empty.
      </span>
      <img src="//c2.staticflickr.com/4/3888/14858236625_e28236b77e_n.jpg" style="max-width:100%;" />
      <br />
      Save the .ppk file and give it a meaningful name<br /><br />
      <img src="//c2.staticflickr.com/6/5574/14671724357_43513dd705.jpg" style="max-width:100%;" />
      <br />
      Now we are ready to connect to our Amazon Instance Machine for the first time.
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Connect to Amazon Instance" id="sourceContent1" style="margin-top:2em;"
                     titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">
          Let’s connect to HadoopNameNode first. Launch Putty client, grab the public URL , import the .ppk private key that we just created for password-less SSH access. 
          As per amazon documentation, for Ubuntu machines username is “ubuntu”
      </span>
    ]]>
    </c:sourceContent>
    
    
    
    <c:sourceContent type="html" title="Provide private key for authentication" id="sourceContent1" style="margin-top:2em;"
        titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
        bodyStyle="background-color:white;padding:0;">
        <![CDATA[
            <img src="//c2.staticflickr.com/4/3851/14688823348_8afbda25c4_b.jpg" style="max-width:100%;" />
        ]]>
    </c:sourceContent>
    
    
    <c:sourceContent type="html" title="Hostname and Port and Connection Type" id="sourceContent1" style="margin-top:2em;"
        titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
        bodyStyle="background-color:white;padding:0;">
        <![CDATA[
            <span style="display:block;margin-bottom:.5em;">
            and “Open” to launch putty session
            </span>
            <img src="//c2.staticflickr.com/4/3850/14875113522_5dfbb47278_n.jpg" style="max-width:100%;" />
            
            <span style="display:block;margin-bottom:.5em;">
            when you launch the session first time, you will see below message, click “Yes”
            </span>
            <img src="//c2.staticflickr.com/4/3917/14875120212_e3c76c6db8_z.jpg" style="max-width:100%;" />
            
            <span style="display:block;margin-bottom:.5em;">
            and will prompt you for the username, enter ubuntu, if everything goes well you will be presented welcome message with Unix shell at the end.
            </span>
            <img src="//c2.staticflickr.com/4/3902/14688856878_c5a0573118.jpg" style="max-width:100%;" />
            
            <span style="display:block;margin-bottom:.5em;">
            If there is a problem with your key, you may receive below error message
            </span>
            <img src="//c2.staticflickr.com/6/5569/14895339223_6e24617b8c.jpg" style="max-width:100%;" />
            
            <span style="display:block;margin-bottom:.5em;">
            Similarly connect to remaining 3 machines HadoopSecondaryNameNode, HaddopSlave1,HadoopSlave2 respectively to make sure you can connect successfully.
            </span>
            <img src="//c2.staticflickr.com/6/5590/14688874738_702481f11f_z.jpg" style="max-width:100%;" />
        ]]>
    </c:sourceContent>


<c:sourceContent type="html" title="Enable Public Access" id="sourceContent1" style="margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
    <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
        Issue ifconfig command and note down the ip address.
        Next, we are going to update the hostname with ec2 public URL and finally we are going to update /etc/hosts file to map  the ec2 public URL with ip address.
        This will help us to configure master ans slaves nodes with hostname instead of ip address.
        <br />
        <br />
        Following is the output on HadoopNameNode ifconfig
        </span>
        <img src="//c2.staticflickr.com/4/3837/14872466211_a458ac6252_z.jpg" style="max-width:100%;" />
        <br />
        <img src="//c2.staticflickr.com/6/5577/14875526425_b02319f0ac_z.jpg" style="max-width:100%;" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        now, issue the hostname command, it will display the ip address same as inet address from ifconfig command.
        </span>
        <img src="//c2.staticflickr.com/4/3837/14688936468_0e43b80cd0.jpg" style="max-width:100%;" />
        
        <span style="display:block;margin-bottom:.5em;">
        We need to modify the hostname to ec2 public URL with below command
        <br />
        <br />
        prebuffer_0nbsp;<b>sudo hostname ec2-54-209-221-112.compute-1.amazonaws.com</b>
        </span>
        <img src="//c2.staticflickr.com/6/5565/14852563996_26b402545d_b.jpg" style="max-width:100%;" />
         ]]>
</c:sourceContent>


<c:sourceContent type="html" title="Modify /etc/hosts" id="sourceContent1" style="margin-top:2em;background-color:white;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
    <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
        Lets change the host to EC2 public IP and hostname.
        <br />
        <br />
        Open the /etc/hosts in vi, in a very first line it will show 127.0.0.1 localhost, we need to replace that with amazon ec2 hostname and ip address we just collected.
        </span>
        <img src="//c2.staticflickr.com/6/5593/14895432933_affce22cc4_b.jpg" style="max-width:100%;" />
        <br />
        <span style="display:block;margin-bottom:.5em;">
        Modify the file and save your changes
        </span>
        <img src="//c2.staticflickr.com/6/5580/14688898320_600fb20f94.jpg" style="max-width:100%;" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        Repeat 2.3 and 2.4 sections for remaining 3 machines.
        </span>
    ]]>
</c:sourceContent>


<c:sourceContent type="html" title="Setup WinSCP access to EC2 instances" id="sourceContent1" style="margin-top:2em;background-color:white;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
    <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
        In order to securely transfer files from your windows machine to Amazon EC2 WinSCP is a handy utility.
        <br />
        <br />
        Provide hostname, username and private key file and save your configuration and Login
        
        </span>
        <img src="//c2.staticflickr.com/6/5563/14852621606_c6523a59a2_c.jpg" style="max-width:100%;" />
        <br />
        <img src="//c2.staticflickr.com/4/3908/14875612135_1551b2393b.jpg" style="max-width:100%;" />
        <br />
        <span style="display:block;margin-bottom:.5em;">
        If you see above error, just ignore and you upon successful login you will see unix file system of a logged in user /home/ubuntu your Amazon EC2 Ubuntu machine.
        
        </span>
        <img src="//c2.staticflickr.com/6/5589/14688990789_f58e490bee_b.jpg" style="max-width:100%;" />
        
        <br />
         <span style="display:block;margin-bottom:.5em;">
        Upload the .pem file to master machine (HadoopNameNode). It will be used while connecting to slave nodes during hadoop startup daemons.
        </span>
    ]]>
</c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>
  
  
  
  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.5em;">
      <c:title>Install and setup Hadoop cluster</c:title>
      <c:desc>
          <c:desc1>
              <![CDATA[
                  <ul style="padding-left:1.5em;list-style:disc;">
                      <li>HadoopNameNode will be referred as <b>master</b></li>
                      <li>HadoopSecondaryNameNode will be referred as <b>SecondaryNameNode</b> or <b>SNN</b></li>
                      <li>HadoopSlave1 and HadoopSlave2 will be referred as <b>slaves</b> (where data nodes will reside)</li>
                  </ul>
              ]]>
          </c:desc1>
      </c:desc>
      <c:sourceContent type="html" title="Install Java" id="sourceContent1" style="background-color:white;"
          titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
          bodyStyle="background-color:white;padding:0;">
          <![CDATA[
              <span style="display:block;margin-bottom:.5em;">
              <b>sudo apt-get update && sudo add-apt-repository ppa:webupd8team/java <br />
              sudo apt-get update && sudo apt-get install oracle-jdk7-installer
              </b>
              <br />
              <br />
              Repeat this for SNN and 2 slaves.
              </span>
          ]]>
      </c:sourceContent>


<c:sourceContent type="html" title="Install Hadoop" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
    <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
        I am going to use haddop 1.2.1 stable version
        <br />
        <br />
        <b>$ wget http://apache.mirror.gtcomm.net/hadoop/common/hadoop-1.2.1/hadoop-1.2.1.tar.gz</b>
        <br />
        <br />
        ...
        </span>
    ]]>
</c:sourceContent>



<c:sourceContent type="html" title="Setup Environment Variable" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
    <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
        $ vi .bashrc<br />
        <br />
        
        Add following at the end of file<br />
        <br />
        
        export HADOOP_CONF=/home/ubuntu/hadoop/conf<br />
        export HADOOP_PREFIX=/home/ubuntu/hadoop<br />
        <br />
        
        #Set JAVA_HOME<br />
        export JAVA_HOME=/usr/lib/jvm/java-7-oracle<br />
        <br />
        
        # Add Hadoop bin/ directory to path<br />
        export PATH=$PATH:$HADOOP_PREFIX/bin<br />
        <br />
        
        
        source ~/.bashrc<br /><br />
        echo $HADOOP_PREFIX<br />
        echo $HADOOP_CONF<br /><br />
        Repeat 1.3 and 1.4  for remaining 3 machines (SNN and 2 slaves).<br />
        </span>
    ]]>
</c:sourceContent>



<c:sourceContent type="html" title="Setup Password-less SSH on Servers" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
    <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
        Master server remotely starts services on salve nodes, whichrequires password-less access to Slave Servers. AWS Ubuntu server comes with pre-installed OpenSSh server.
        <br />
        <br />
        Quick Note:<br />
        The public part of the key loaded into the agent must be put on the target system in ~/.ssh/authorized_keys. This has been taken care of by the AWS Server creation process <br />
        <br />
        Now we need to add the AWS EC2 Key Pair identity haddopec2cluster.pem to SSH profile. In order to do that we will need to use following ssh utilities
        <br />
        <br />
        <b>‘ssh-agent’</b> is a background program that handles passwords for SSH private keys. <br />
        <b>‘ssh-add’</b> command prompts the user for a private key password and adds it to the list maintained by ssh-agent.
        <br />
        <br />
        Once you add a password to ssh-agent, you will not be asked to provide the key when using SSH or SCP to connect to hosts with your public key.
        <br />
        <br />
        
        Amazon EC2 Instance  has already taken care of ‘authorized_keys’ on master server, execute following commands to allow password-less SSH access to slave servers.
        <br />
        <br />
        
        First of all we need to protect our keypair files, if the file permissions are too open (see below) you will get an error
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/4/3909/14875817675_768300bd57_z.jpg" />
        <br />
        <span style="display:block;margin-bottom:.5em;">
          To fix this problem, we need to issue following commands <br />
          <br />

          <b>$ chmod 644 authorized_keys</b>
          <br />
          <br />
          <b>Quick Tip: </b>
          <br />
          If you set the permissions to ‘chmod 644′, you get a file that can be written by you, but can only be read by the rest of the world.
          <br />
          <br />

          <b>$ chmod 400 haddoec2cluster.pem</b>
          <br />
          <br />

          <b>Quick Tip: </b>
          <br />chmod 400 is a very restrictive setting giving only the file onwer read-only access. No write / execute capabilities for the owner, and no permissions what-so-ever for anyone else.
          <br />
          <br />

          To use ssh-agent and ssh-add, follow the steps below:
          <br />
          <br />

          1. At the Unix prompt, enter: <br />
          <b>eval `ssh-agent`</b>
          <br />
          Note: Make sure you use the backquote ( ` ), located under the tilde ( ~ ), rather than the single quote ( ' ).
          <br />
          <br />
          2. Enter the command: <br />
          <b>ssh-add hadoopec2cluster.pem</b>
          <br />
          <br />
          if you notice .pem file has “read-only” permission now and this time it works for us.
         </span>
                 <img style="max-width:100%;" src="//c2.staticflickr.com/6/5582/14884283275_28c8e80050_z.jpg" />
        <br />
        <span style="display:block;margin-bottom:.5em;">
            Keep in mind ssh session will be lost upon shell exit and you have repeat ssh-agent and ssh-add commands.
        </span>
    ]]>
</c:sourceContent>

    <c:sourceContent type="html" title="Remote SSH" id="sourceContent1" style="background-color:white;margin-top:2em;"
        titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
        bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        
        <span style="display:block;margin-bottom:.5em;">
         Let’s verify that we can connect into SNN and slave nodes from master
         </span>
                 <img style="max-width:100%;" src="//c2.staticflickr.com/6/5554/14883977732_76396619eb_b.jpg" />
        <br />
        <span style="display:block;margin-bottom:.5em;">
           <b> $ ssh ubuntu@&lt;your-amazon-ec2-public URL&gt;</b>
           <br />
            On successful login the IP address on the shell will change.
        </span>
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title=" Hadoop Cluster Setup" id="sourceContent1" style="background-color:white;margin-top:2em;"
        titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
        bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        
        <span style="display:block;margin-bottom:.5em;">
         This section will cover the hadoop cluster configuration.  We will have to modify
         <br />
         <br />
         <ul style="padding-left:1.5em;list-style:disc;">
            <li>
                <b>hadoop-env.sh</b> <br />This file contains some environment variable settings used by Hadoop. You can use these to affect some aspects of Hadoop daemon behavior, such as where log files are stored, the maximum amount of heap used etc. The only variable you should need to change at this point is in this file is JAVA_HOME, which specifies the path to the Java 1.7.x installation used by Hadoop.
            </li>
            <li><b>core-site.xml</b> <br />key property fs.default.name – for namenode configuration for e.g hdfs://namenode/</li>
            <li><b>hdfs-site.xml</b> <br />key property – dfs.replication – by default 3</li>
            <li><b>mapred-site.xml</b>  <br />key property  mapred.job.tracker for jobtracker configuration for e.g jobtracker:8021</li>
         </ul>
          <br />
          <br />
          We will first start with master (NameNode) and then copy above xml changes to remaining 3 nodes (SNN and slaves)
          <br />
          <br />

          Finally, in section 1.6.2 we will have to configure conf/masters and conf/slaves.
          <br />
          <br />

          <ul style="padding-left:1.5em;list-style:disc;">
              <li><b>masters</b> – defines on which machines Hadoop will start secondary NameNodes in our multi-node cluster.</li>
              <li><b>slaves</b> –  defines the lists of hosts, one per line, where the Hadoop slave daemons (datanodes and tasktrackers) will run.</li>
          </ul>
          <br />
          <br />
          Lets go over one by one. Start with masters (namenode).
          <br />
          <br />

          <b>hadoop-env.sh</b>
          <br />

          <b>$ vi $HADOOP_CONF/hadoop-env.sh</b>  and add JAVA_HOME shown below and save changes.
         </span>
         
          <img style="max-width:100%;" src="//c2.staticflickr.com/6/5588/14883989642_39447c4c8d_b.jpg" />
          <br />
          
          
          <span style="display:block;margin-bottom:.5em;">
           <b>core-site.xml</b>
           <br />

          This file contains configuration settings for Hadoop Core (for e.g I/O) that are common to HDFS and MapReduce Default file system configuration property – fs.default.name  goes here it could for e.g hdfs / s3 which will be used by clients.
          <br />
          <br />
          <b>$ vi $HADOOP_CONF/core-site.xml</b>
          <br />
          <br />
          We are going t0 add two properties
          <br />
          <br />
          <ul style="padding-left:1.5em;list-style:disc;">
            <li><b>fs.default.name</b>  <br />
            will point to NameNode URL and port (usually 8020)</li>
            <li><b>hadoop.tmp.dir</b>  <br />
            A base for other temporary directories. Its important to note that every node needs hadoop tmp directory.  I am going to create a new directory “hdfstmp”  as below in all 4 nodes. Ideally you can write a shell script to do this for you, but for now going the manual way.</li>
          </ul>
          <br />
          <br />
          <b>
          $ cd
          <br />

          $ mkdir hdfstmp
          
          </b>
          
          <br />
          <br />
          <b>Quick Tip:  </b>
          <br />
          Some of the important directories are <b>dfs.name.dir</b>, <b>dfs.data.dir</b> in <b>hdfs-site.xml</b>. 
          <br />
          The default value for the <b>dfs.name.dir</b> is <b>${hadoop.tmp.dir}/dfs/data</b> and <b>dfs.data.dir</b> is <b>${hadoop.tmp.dir}/dfs/data</b>. 
          <br /> 
          It is critical that you choose your directory location wisely in production environment.
          <br />
          <br />
          &lt;configuration&gt; <br />
          &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &lt;property&gt; <br />
          &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    &lt;name&gt;fs.default.name&lt;/name&gt; <br />
          &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    &lt;value&gt;hdfs://ec2-54-209-221-112.compute-1.amazonaws.com:8020&lt;/value&gt; <br />
          &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &lt;/property&gt; <br /> <br />

          &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &lt;property&gt; <br />
          &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; <br />
          &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;    &lt;value&gt;/home/ubuntu/hdfstmp&lt;/value&gt; <br />
          &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  &lt;/property&gt; <br />
          &lt;/configuration&gt;
        </span>
        
        <br />
        <br />
        
          <span style="display:block;margin-bottom:.5em;">
           <b>hdfs-site.xml</b>
           <br />
          This file contains the configuration for HDFS daemons, the NameNode, SecondaryNameNode  and data nodes.
          <br />
          <br />

          We are going to add 2 properties
          <br />
          <ul style="padding-left:1.5em;list-style:disc;">
              <li><b>dfs.permissions.enabled</b>  <br />
              with value false,  This means that any user, not just the “hdfs” user, can do anything they want to HDFS so do not do this in production unless you have a very good reason. if “true”, enable permission checking in HDFS. If “false”, permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. Be very careful before you set this</li>
              <li><b>dfs.replication</b><br />
              Default block replication is 3. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time. Since we have 2 slave nodes we will set this value to 2.</li>
          </ul>
           <br />
          &lt;configuration&gt; <br />
          &nbsp;&nbsp;&nbsp;&nbsp; &lt;property&gt; <br />
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;dfs.replication&lt;/name&gt; <br />
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;2&lt;/value&gt; <br />
          &nbsp;&nbsp;&nbsp;&nbsp;&lt;/property&gt; <br />
          &nbsp;&nbsp;&nbsp;&nbsp;&lt;property&gt; <br />
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;dfs.permissions&lt;/name&gt; <br />
          &nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;false&lt;/value&gt; <br />
          &nbsp;&nbsp;&nbsp;&nbsp;&lt;/property&gt; <br />
          &lt;/configuration&gt;
          
        </span>
        
          <img style="max-width:100%;" src="//c2.staticflickr.com/4/3896/14697895827_868cf49e44_z.jpg" />
          <br />
          <br /><br />

          <span style="display:block;margin-bottom:.5em;">
           <b>mapred-site.xml</b>
           <br />
          This file contains the configuration settings for MapReduce daemons: the job tracker and the task-trackers.
          <br />
          The mapred.job.tracker parameter is a hostname (or IP address) and port pair on which the Job Tracker listens for RPC communication. 
          This parameter specify the location of the Job Tracker for Task Trackers and MapReduce clients.
          <br />
          <br />
          JobTracker will be running on master (NameNode)
          <br />

          &lt;configuration&gt;<br />
          &nbsp;&nbsp;&nbsp;&nbsp;&lt;property&gt;<br />
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;name&gt;mapred.job.tracker&lt;/name&gt;<br />
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&lt;value&gt;hdfs://ec2-54-209-221-112.compute-1.amazonaws.com:8021&lt;/value&gt;<br />
          &nbsp;&nbsp;&nbsp;&nbsp;&lt;/property&gt;<br />
          &lt;/configuration&gt;<br />
          
        </span>
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Move configuration files to Slaves" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
        Now, we are done with hadoop xml files configuration master, lets copy the files to remaining 3 nodes using secure copy (scp)
        <br />
        <br />
        start with SNN, if you are starting a new session, follow ssh-add as per  <b>(Setup Password-less SSH on Servers)</b>
        <br />
        <br />
        from master’s unix shell issue below command
        <br />
        <br />
        $ scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml ubuntu@ec2-54-209-221-47.compute-1.amazonaws.com:/home/ubuntu/hadoop/conf
        <br />
        <br />
        repeat this for slave nodes
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/4/3877/14884160022_e158d1959c_z.jpg" />
        <br />
        
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Configure Master and Slaves" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
            Every hadoop distribution comes with master and slaves files. 
            By default it contains one entry for localhost,
            we have to modify these 2 files on both “masters” (HadoopNameNode) and “slaves” (HadoopSlave1 and HadoopSlave2) machines – we have a dedicated machine for HadoopSecondaryNamdeNode.
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/4/3925/14884514275_f87f516269_z.jpg" />
        <br />
        <img style="max-width:100%;" src="//c2.staticflickr.com/6/5560/14881480791_a83620ca34.jpg" />
        
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Modify masters file on Master machine" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
            conf/masters file defines on which machines Hadoop will start Secondary NameNodes in our multi-node cluster. In our case, there will be two machines HadoopNameNode and HadoopSecondaryNameNode
            <br />
            <br />
            <b>Hadoop HDFS user guide :</b>
            <br />
           <blockquote> “The secondary NameNode merges the fsimage and the edits log files periodically and keeps edits log size within a limit. It is usually run on a different machine than the primary NameNode since its memory requirements are on the same order as the primary NameNode. The secondary NameNode is started by “bin/start-dfs.sh“ on the nodes specified in “conf/masters“ file.“</blockquote>
            <br />
            $ <b>vi $HADOOP_CONF/masters</b> and provide an entry for the hostename where you want to run SecondaryNameNode daemon. In our case HadoopNameNode and HadoopSecondaryNameNode
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/4/3917/14697865530_7f346fa6ea.jpg" />
        <br />
        
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="MODIFY THE SLAVES FILE ON MASTER MACHINE" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
            The slaves file is used for starting DataNodes and TaskTrackers
            <br />
            <br />
            <b>$ vi $HADOOP_CONF/slaves</b>
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/4/3898/14884550735_b7ce17127c_b.jpg" />
        <br />
        
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Copy masters and slaves to SecondaryNameNode" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
            Since SecondayNameNode configuration will be same as NameNode, we need to copy master and slaves to HadoopSecondaryNameNode.
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/4/3906/14904437303_006e8bd482_b.jpg" />
        <br />
        
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="CONFIGURE MASTER AND SLAVES ON “SLAVES” NODE" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
            Since we are configuring slaves (HadoopSlave1 & HadoopSlave2) , masters file on slave machine is going to be empty
            <br />
            <br />
            <b>$ vi $HADOOP_CONF/masters</b>
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/6/5594/14881539651_a7886295cf_z.jpg" />
        <br />
        <span style="display:block;margin-bottom:.5em;">
            Next, update the ‘slaves’ file on Slave server (HadoopSlave1) with the IP address of the slave node. Notice that the ‘slaves’ file at Slave node contains only its own IP address and not of any other Data Node in the cluster.
            <br />
            <br />
            <b>$ vi $HADOOP_CONF/slaves</b>
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/6/5569/14697946619_ab517cd357.jpg" />
        <br />
        <span>Similarly update masters and slaves for HadoopSlave2</span>
    ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="Hadoop Daemon Startup" id="sourceContent1" style="background-color:white;margin-top:2em;"
    titleStyle="padding-left:0;color:black;font-size:1.3em;background-color:white;"
    bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.5em;">
            The first step to starting up your Hadoop installation is formatting the Hadoop filesystem which runs on top of your , which is implemented on top of the local filesystems of your cluster. You need to do this the first time you set up a Hadoop installation. Do not format a running Hadoop filesystem, this will cause all your data to be erased.
            <br />
            <br />
            To format the namenode
            <br />
            <br />
            <b>$ hadoop namenode format</b>
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/6/5594/14697928820_4fc9ccaac4_c.jpg" />
        <br />
        <span style="display:block;margin-bottom:.5em;">
          Lets start all hadoop daemons from HadoopNameNode
          <br />
            <br />
          $ cd $HADOOP_CONF
          <br />
            <br />
          $ start-all.sh
          This will start
          <br />
            <br />
          NameNode,JobTracker and SecondaryNameNode daemons on HadoopNameNode
        </span>
        <img style="max-width:100%;" src="//c2.staticflickr.com/6/5593/14697937330_3230d93cc0_b.jpg" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        <b>SecondaryNameNode daemons on HadoopSecondaryNameNode</b>
        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/4/3915/14881581391_bc27b89017_b.jpg" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        <b>and DataNode and TaskTracker daemons on slave nodes HadoopSlave1 and HadoopSlave2</b>
        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/4/3920/14698092547_9c6c6278d8_b.jpg" />
        <br />
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/4/3902/14882176764_8ba0bb2edc_b.jpg" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        <b>We can check the namenode status from http://ec2-54-209-221-112.compute-1.amazonaws.com:50070/dfshealth.jsp</b>
        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/4/3869/14698030248_7db5f8d0b8_b.jpg" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        Check Jobtracker status : http://ec2-54-209-221-112.compute-1.amazonaws.com:50030/jobtracker.jsp
        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/4/3888/14882191944_e9542f6bf6_b.jpg" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        Slave Node Status for HadoopSlave1 : http://ec2-54-209-223-7.compute-1.amazonaws.com:50060/tasktracker.jsp
        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/4/3910/14698004609_e43f02ae6d_b.jpg" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        Slave Node Status for HadoopSlave2 : http://ec2-54-209-219-2.compute-1.amazonaws.com:50060/tasktracker.jsp
        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/4/3913/14697979930_8e62ac0b4d_b.jpg" />
        <br />
        <br />
        <br />
        
        
        <span style="display:block;margin-bottom:.5em;font-size:2em;color:rgb(175, 0, 190);">
        To quickly verify our setup, run the hadoop pi example
        </span>
        <span style="display:block;margin-bottom:.5em;">
        <br />
        <br />
        <b>ubuntu@ec2-54-209-221-112:~/hadoop$ hadoop jar hadoop-examples-1.2.1.jar pi 10 1000000</b>
        <br />
        <br />
        You can check the job tracker status page to look at complete job status
        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/4/3841/14904537343_f22bd9eba1_b.jpg" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        Drill down into completed job and you can see more details on Map Reduce tasks.

        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/6/5571/14882224304_01c86ca7c1_b.jpg" />
        <br />
        
        <span style="display:block;margin-bottom:.5em;">
        At last do not forget to terminate your amazon ec2 instances or you will be continued to get charged
        </span>
        <img style="max-width:100%;border:solid 2px red;" src="//c2.staticflickr.com/6/5560/14882230694_1f1582f623_b.jpg" />
        <br />
        <span>That’s it for this article, hope you find it useful
        <br />
        Happy Hadoop Year!</span>
    ]]>
    </c:sourceContent>
    
    <c:comment>
    <c:comment1>
        <![CDATA[]]>
    </c:comment1>
</c:comment>
  </c:entry>
  
</c:component>
