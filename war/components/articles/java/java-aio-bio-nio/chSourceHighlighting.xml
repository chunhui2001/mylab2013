<?xml version="1.0" encoding="utf-8"?>
<c:component type="chSourceHighlighting" componentId="chSourceHighlighting_1" xmlns:c="http://com.snnmo.website">
  <c:abstract>
    <![CDATA[

<div>
    Java IO 体系看起来类很多，感觉很复杂，但其实是 IO 涉及的因素太多了。在设计 IO 相关的类时，编写者也不是从同一个方面考虑的，所以会给人一种很乱的感觉，并且还有设计模式的使用，更加难以使用这些 IO 类，所以特地对 Java 的 IO 做一个总结。
</div>
<div style="margin-top:.625em;">
IO 类设计出来，肯定是为了解决 IO 相关的操作的，想一想哪里会有 IO 操作？网络、磁盘。网络操作相关的类是在 java.net 包下，不在本文的总结范围内。提到磁盘，你可能会想到文件，文件操作在 IO 中是比较典型的操作。在 Java 中引入了 “流” 的概念，它表示任何有能力产生数据源或有能力接收数据源的对象。数据源可以想象成水源，海水、河水、湖水、一杯水等等。数据传输可以想象为水的运输，古代有用桶运水，用竹管运水的，现在有钢管运水，不同的运输方式对应不同的运输特性。
</div>

<div style="margin-top:.625em;">
    <img style="margin:auto;width:100%;" src="/images/java/java-io.webp" />
<div>

<div style="margin-top:.625em;">
在高性能的I/O设计中，有两个比较著名的模式Reactor和Proactor模式，其中Reactor模式用于同步I/O，而Proactor运用于异步I/O操作。
</div>

<div style="margin-top:.625em;">
在比较这两个模式之前，我们首先的搞明白几个概念，什么是<b style="color:blue;">阻塞和非阻塞</b>，什么是<b style="color:green;">同步和异步</b>, 同步和异步是针对应用程序和内核的交互而言的，<b style="color:green;">同步指的是用户进程触发IO操作并等待或者轮询的去查看IO操作是否就绪，而异步是指用户进程触发IO操作以后便开始做自己的事情，而当IO操作已经完成的时候会得到IO完成的通知</b>。而<b style="color:blue;">阻塞和非阻塞是针对于进程在访问数据的时候，根据IO操作的就绪状态来采取的不同方式</b>，说白了是一种读取或者写入操作函数的实现方式，阻塞方式下读取或者写入函数将一直等待，而非阻塞方式下，读取或者写入函数会立即返回一个状态值。
</div>


<h4 style="color:red;margin-top:1em;">一般来说I/O模型可以分为：同步阻塞，同步非阻塞，异步阻塞，异步非阻塞IO</h4>

<div style="margin-top:.625em;">
▪▪ <b>同步阻塞IO:</b> 在此种方式下，用户进程在发起一个IO操作以后，必须等待IO操作的完成，只有当真正完成了IO操作以后，用户进程才能运行。JAVA传统的IO模型属于此种方式！
</div>

<div style="margin-top:.625em;">
▪▪ <b>同步非阻塞IO:</b> 在此种方式下，用户进程发起一个IO操作以后边可返回做其它事情，但是用户进程需要时不时的询问IO操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的CPU资源浪费。其中目前JAVA的NIO就属于同步非阻塞IO。
</div>

<div style="margin-top:.625em;">
▪▪ <b>异步阻塞IO:</b> 此种方式下是指应用发起一个IO操作以后，不等待内核IO操作的完成，等内核完成IO操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问IO是否完成，那么为什么说是阻塞的呢？因为此时是通过 select 系统调用来完成的，而 select 函数本身的实现方式是阻塞的，而采用 select 函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！
</div>

<div style="margin-top:.625em;">
▪▪ <b>异步非阻塞IO:</b> 在此种模式下，用户进程只需要发起一个IO操作然后立即返回，等IO操作真正的完成以后，应用程序会得到IO操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的IO读写操作，因为真正的IO读取或者写入操作已经由内核完成了。目前Java中还没有支持此种IO模型。 
</div>

]]>
  </c:abstract>


  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.2em;">
    <c:title><![CDATA[Java &mdash; IO 类]]></c:title>
    <c:desc>
      <c:desc1> <![CDATA[
       
      ]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[


<ul style="margin-left: 1.5em;list-style-type: disc;margin-top: .3em;">
    <li>文件（file）：FileInputStream、FileOutputStream、FileReader、FileWriter</li>
    <li>字节数组（byte[]）：ByteArrayInputStream、ByteArrayOutputStream</li>
    <li>字符数组（char[]）：CharArrayReader、CharArrayWriter</li>
    <li>管道操作：PipedInputStream、PipedOutputStream、PipedReader、PipedWriter</li>
    <li>基本数据类型：DataInputStream、DataOutputStream</li>
    <li>缓冲操作：BufferedInputStream、BufferedOutputStream、BufferedReader、BufferedWriter</li>
    <li>打印：PrintStream、PrintWriter</li>
    <li>对象序列化反序列化：ObjectInputStream、ObjectOutputStream</li>
    <li>转换：InputStreamReader、OutputStreWriter</li>
    <li style="text-decoration:line-through;">字符串（String）Java8中已废弃：StringBufferInputStream、StringBufferOutputStream、StringReader、StringWriter</li>
</ul>
]]>
    </c:sourceContent>


    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>


  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.2em;">
    <c:title><![CDATA[AIO、BIO、NIO 区别]]></c:title>
    <c:desc>
      <c:desc1> <![CDATA[
<h4 style="color:green;">Java对BIO、NIO、AIO的支持</h4>
<div style="margin-top:.625em;">
  <b>Java BIO:</b> 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。
</div>
<div style="margin-top:.625em;">
  <b>Java NIO:</b> 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。
</div>
<div style="margin-top:.625em;">
  <b>Java AIO(NIO.2):</b> 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理。
</div>

<h4 style="color:green;margin-top:1.5em;">BIO、NIO、AIO适用场景分析</h4>
<div style="margin-top:.625em;">
  BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。
</div>
<div style="margin-top:.625em;">
  NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。
</div>
<div style="margin-top:.625em;">
  AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。
</div>

      ]]></c:desc1>
    </c:desc>
    <c:sourceContent type="" title="bio 同步阻塞" id="sourceContent1" 
                     style="background-color:white;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
ServerSocket serverSocket = new ServerSocket(10101);

while (true) {
   // 获取一个套接字(阻塞) 
   final Socket socket = serverSocket.accept();

   // 开个线程, 处理连接的 clint socket... new

   // 此时接受客户端连接的线程(请求线程), 一直处于阻塞状态, 等待客户端连接(阻塞),
   // 每个 client 连接后分配一个线程 (用户线程(进程线程))(同步) --> 同步阻塞
   // ... 
}

]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.625em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div style="margin-top:.625em;">
在JDK1.4出来之前，我们建立网络连接的时候采用BIO模式，需要先在服务端启动一个ServerSocket，然后在客户端启动Socket来与服务端进行通信，默认情况下服务端需要对每个请求建立一堆线程等待请求，而客户端发送请求后，先咨询服务端是否有线程响应，如果没有则会一直等待或者遭到拒绝请求，如果有的话，客户端会线程会等待请求结束后才继续执行。
</div>
]]>
    </c:sourceContent>
    <c:sourceContent type="" title="nio 同步非阻塞" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

SocketChannel serverChannel = SocketChannel.open();

selector = Selector.open();
serverChannel.socket().bind(new InetSocketAddress(8080));
serverChannel.configureBlocking(false);
serverChannel.register(selector, SelectionKey.OP_ACCEPT);

// selector.selectedKeys() 一直轮询着, 如果事件处理很费时间, 依旧是阻塞的
// 也需要要开线程去异步处理, 保证请求的主线程不被阻塞, 达到非阻塞目的, 
// 所以它可以被设计成阻塞的, 也可以被设计成非阻塞的, 他也是基于进程线程的读写,
// 所以属于同步 --> 综上可得到他是同步非阻塞
Set<SelectionKey> sleSet = selector.selectedKeys();

while (true) {

   int count = selector.select();
   Iterator<SelectionKey> iterator = sleSet.iterator();

   while (iterator.hasNext()) {

       SelectionKey selectionKey = iterator.next();
       Buffer buffer = ByteBuffer.allocate(1024);

       if(selectionKey.isAcceptable()) {
           ServerSocketChannel socketChannel = (ServerSocketChannel) selectionKey.channel();
           // 处理逻辑,开线程

       } else if(selectionKey.isReadable()){

       } else if (selectionKey.isWritable()){

       }

       iterator.remove();
   }
}

]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.625em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  NIO本身是基于事件驱动思想来完成的，其主要想解决的是BIO的大并发问题：在使用同步I/O的网络应用中，如果要同时处理多个客户端请求，或是在客户端要同时和多个服务器进行通讯，就必须使用多线程来处理。也就是说，将每一个客户端请求分配给一个线程来单独处理。这样做虽然可以达到我们的要求，但同时又会带来另外一个问题。由于每创建一个线程，就要为这个线程分配一定的内存空间（也叫工作存储器），而且操作系统本身也对线程的总数有一定的限制。如果客户端的请求过多，服务端程序可能会因为不堪重负而拒绝客户端的请求，甚至服务器可能会因此而瘫痪。
</div>
<div style="margin-top:.625em;">
  NIO基于Reactor，当socket有流可读或可写入socket时，操作系统会相应的通知引用程序进行处理，应用再将流读取到缓冲区或写入操作系统。也就是说，这个时候，已经<em style="font-size:1.125em;color:red;">不是一个连接就要对应一个处理线程了，而是有效的请求对应一个线程，当连接没有数据时，是没有工作线程来处理的</em>。
</div>
<div style="margin-top:.625em;">
  <em style="font-size:1.125em;color:blue;">BIO与NIO一个比较重要的不同，是我们使用BIO的时候往往会引入多线程，每个连接一个单独的线程；而NIO则是使用单线程或者只使用少量的多线程，多个连接共用一个线程。</em>
</div>
<div style="margin-top:.625em;">
  NIO的最重要的地方是当一个连接创建后，不需要对应一个线程，这个连接会被注册到多路复用器上面，所以所有的连接只需要一个线程就可以搞定，当这个线程中的多路复用器进行轮询的时候，发现连接上有请求的话，才开启一个线程进行处理，也就是一个请求一个线程模式。
</div>
<div style="margin-top:.625em;">
  在NIO的处理方式中，当一个请求来的话，开启线程进行处理，可能会等待后端应用的资源(JDBC连接等)，其实这个线程就被阻塞了，当并发上来的话，还是会有BIO一样的问题。
</div>
<div style="margin-top:.625em;">
  HTTP/1.1出现后，有了Http长连接，这样除了超时和指明特定关闭的http header外，这个链接是一直打开的状态的，这样在NIO处理中可以进一步的进化，在后端资源中可以实现资源池或者队列，<em style="font-size:1.125em;color:green;">当请求来的时候，开启的线程把请求和请求数据传送给后端资源池或者队列里面就返回，并且在全局的地方保持住这个现场(哪个连接的哪个请求等)，这样前面的线程还是可以去接受其他的请求，而后端的应用的处理只需要执行队列里面的就可以了，这样请求处理和后端应用是异步的. 当后端处理完，到全局地方得到现场并产生响应，这个就实现了异步处理</em>。
</div>
]]>
    </c:sourceContent>

    <c:sourceContent type="" title="aio 异步非阻塞" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
// 基于事件的回调, future/callback, 大概的过程是操作系统内核线程处理完, 通知进程线程拿到结果, 
// 期间进程线程可以做其他的事情, 这样做的好处是充分利用操作系统的并发能力.

]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.625em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  与NIO不同，当进行读写操作时，只须直接调用API的read或write方法即可。这两种方法均为异步的，对于读操作而言，当有流可读取时，操作系统会将可读的流传入read方法的缓冲区，并通知应用程序；对于写操作而言，当操作系统将write方法传递的流写入完毕时，操作系统主动通知应用程序。即可以理解为，read/write方法都是异步的，完成后会主动调用回调函数。  在JDK1.7中，这部分内容被称作NIO.2，主要在java.nio.channels包下增加了下面四个异步通道：
</div>
<pre style="font-size:.625em;padding:1em;background-color:beige;color:green;margin-top:.325em;border:solid 1px cadetblue;">
  AsynchronousSocketChannel
  AsynchronousServerSocketChannel
  AsynchronousFileChannel
  AsynchronousDatagramChannel
</pre>
]]>
    </c:sourceContent>


    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>



  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.2em;">
    <c:title><![CDATA[网络编程 select, poll, epoll 区别于联系]]></c:title>
    <c:desc>
      <c:desc1> <![CDATA[
<div>
select，poll，epoll 都是IO多路复用中的模型。同其他IO的不同的是, IO多路复用一次可以等多个文件描述符。大大提高了等待数据准备好的时间的效率。以下是一组多路复用的模型图示:
</div>

<div style="margin-top:.325em;">
  <img style="width:680px;" src="/images/java/select.png"></img>
</div>

      ]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="select 的缺点" id="sourceContent1" 
                     style="background-color:white;"
                     titleStyle="padding-left:0;color:blue;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  <b style="color:green;">单个进程监控的文件描述符有限，通常为1024*8个文件描述符。</b>当然可以改进，由于 select 采用轮询方式扫描文件描述符。文件描述符数量越多，性能越差。
</div>
<div style="margin-top:.625em;">
  <b style="color:green;">内核/用户数据拷贝频繁，操作复杂。</b>select 在调用之前，需要手动在应用程序里将要监控的文件描述符添加到 fed_set 集合中。然后加载到内核进行监控。用户为了检测时间是否发生，还需要在用户程序手动维护一个数组，存储监控文件描述符。当内核事件发生，在将 fed_set 集合中没有发生的文件描述符清空，然后拷贝到用户区，和数组中的文件描述符进行比对。再调用 selecct 也是如此。每次调用，都需要来回拷贝。
</div>
<div style="margin-top:.625em;">
  <b style="color:green;">轮询时间效率低。</b>select 返回的是整个数组的句柄。应用程序需要遍历整个数组才知道谁发生了变化。轮询代价大。
</div>
<div style="margin-top:.625em;">
  <b style="color:green;">select 是水平触发。</b>应用程序如果没有完成对一个已经就绪的文件描述符进行IO操作。那么之后 select 调用还是会将这些文件描述符返回，通知进程。
</div>
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="poll 特点" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:blue;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  poll 操作比 select 稍微简单点。select 采用三个位图来表示 fd_set，poll 使用 pollfd 的指针. pollfd 结构包含了要监视的 event 和发生的 event，不再使用 select 传值的方法。更方便~
</div>
<div style="margin-top:.625em;">
  <b style="color:green;">select 的缺点依然存在。</b>拿 select 为例，假如我们的服务器需要支持100万的并发连接。则在 FD_SETSIZE 为1024的情况下，我们需要开辟100个并发的进程才能实现并发连接。除了进程上下调度的时间消耗外。从内核到用户空间的无脑拷贝，数组轮询等，也是系统难以接受的。因此，基于 select 实现一个百万级别的并发访问是很难实现的。
</div>
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="epoll模型" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:blue;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  由于epoll和上面的实现机制完全不同，所以上面的问题将在epoll中不存在。
</div>
<div style="margin-top:.625em;">
  在select/poll中，服务器进程每次调用select都需要把这100万个连接告诉操作系统（从用户态拷贝到内核态）。让操作系统检测这些套接字是否有时间发生。轮询完之后，再将这些句柄数据复制到操作系统中，让服务器进程轮询处理已发生的网络时间。这一过程耗时耗力，而epoll通过在linux申请一个建议的文件系统，把select调用分为了三部分。
</div>

<ul style="margin-left:.5em;margin-top:.625em;line-height:1.425em;">
    <li style="margin-bottom: .3em;">
      &#x278A; 调用epoll_create建立一个epoll对象，这个对象包含了一个红黑树和一个双向链表。并与底层建立回调机制。
    </li>
    <li style="margin-bottom: .3em;">
      &#x278B; 调用epoll_ctl向epoll对象中添加这100万个连接的套接字
    </li>
    <li style="margin-bottom: .3em;">
     &#x278C; 调用epoll_wait收集发生事件的连接。
    </li>
</ul>

<div style="margin-top:.625em;">
  从上面的调用方式就可以看到epoll比select/poll的优越之处：因为后者每次调用时都要传递你所要监控的所有socket给select/poll系统调用，这意味着需要将用户态的socket列表copy到内核态，如果以万计的句柄会导致每次都要copy几十几百KB的内存到内核态，非常低效。而我们调用epoll_wait时就相当于以往调用select/poll，但是这时却不用传递socket句柄给内核，因为内核已经在epoll_ctl中拿到了要监控的句柄列表。
</div>

<div style="margin-top:.625em;">
  所以，实际上在你调用epoll_create后，内核就已经在内核态开始准备帮你存储要监控的句柄了，每次调用epoll_ctl只是在往内核的数据结构里塞入新的socket句柄。
</div>

<div style="margin-top:.625em;">
  在内核里，一切皆文件。所以，epoll向内核注册了一个文件系统，用于存储上述的被监控socket。当你调用epoll_create时，就会在这个虚拟的epoll文件系统里创建一个file结点。当然这个file不是普通文件，它只服务于epoll。
</div>

<div style="margin-top:.625em;">
  epoll在被内核初始化时（操作系统启动），同时会开辟出epoll自己的内核高速cache区，用于安置每一个我们想监控的socket，<em style="font-size:1.125em;color:green;">这些socket会以红黑树的形式保存在内核cache里</em>，以支持快速的查找、插入、删除。这个内核高速cache区，就是建立连续的物理内存页，然后在之上建立slab层，简单的说，就是物理上分配好你想要的size的内存对象，每次使用时都是使用空闲的已分配好的对象。
</div>

<div style="margin-top:.625em;">
  epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们用户。这是由于<em style="font-size:1.125em;color:chocolate;">我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。</em>所以，epoll_wait非常高效。
</div>

<div style="margin-top:.625em;">
  而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已，如何能不高效？！
</div>

<div style="margin-top:.625em;">
  那么，这个准备就绪list链表是怎么维护的呢？<em style="font-size:1.125em;color:darkorchid;">当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。</em>
</div>

<div style="margin-top:.625em;">
  如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。
</div>


<h4 style="color:green;margin-top:1em;">两种模式LT和ET</h4>

<div style="margin-top:.625em;">
  最后看看epoll独有的两种模式LT和ET。无论是LT和ET模式，都适用于以上所说的流程。区别是，LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时次次返回这个句柄，而ET模式仅在第一次返回。
</div>

<div style="margin-top:.625em;">
  这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。
</div>


]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:blue;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<div>
epoll 用 kmem_cache_create（slab分配器）分配内存用来存放 struct epitem 和 struct eppoll_entry。
</div>

<div style="margin-top:.325em;">
当向系统中添加一个 fd 时，就创建一个 epitem 结构体，这是内核管理 epoll 的基本数据结构：
</div>

]]>
    </c:sourceContent>

    <c:sourceContent type="" title="epitem" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
struct epitem {
  struct rb_node rbn;       // 用于主结构管理的红黑树
  struct list_head rdllink; // 事件就绪队列
  struct epitem *next;      // 用于主结构体中的链表
  struct epoll_filefd ffd;  // 这个结构体对应的被监听的文件描述符信息
  int nwait;                // poll操作中事件的个数
  struct list_head pwqlist; // 双向链表，保存着被监视文件的等待队列，功能类似于select/poll中的poll_table
  struct eventpoll *ep;     // 该项属于哪个主结构体（多个epitm从属于一个eventpoll）
  struct list_head fllink;  // 双向链表，用来链接被监视的文件描述符对应的struct file。因为file里有f_ep_link,用来保存所有监视这个文件的epoll节点
  struct epoll_event event; // 注册的感兴趣的事件,也就是用户空间的epoll_event
}
]]>
    </c:sourceContent>

    <c:sourceContent type="" title="eventpoll" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
struct eventpoll {
  spin_lock_t lock;            // 对本数据结构的访问
  struct mutex mtx;            // 防止使用时被删除
  wait_queue_head_t wq;        // sys_epoll_wait() 使用的等待队列
  wait_queue_head_t poll_wait; // file->poll() 使用的等待队列
  struct list_head rdllist;    // 事件满足条件的链表 /* 双链表中则存放着将要通过 epoll_wait 返回给用户的满足条件的事件 */
  struct rb_root rbr;          // 用于管理所有fd的红黑树（树根）/* 红黑树的根节点，这颗树中存储着所有添加到 epoll 中的需要监控的事件 */
  struct epitem *ovflist;      // 将事件到达的fd进行链接起来发送至用户空间
}
]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div style="margin-top:.325em;">
  <div style="width:49%;float:left;margin-right:2%;">
    <h6 style="color:green;">struct eventpoll 在 epoll_create 时创建。所以内核中维护了一棵红黑树，大致的结构如下：</h6>
    <img style="width:100%" src="/images/java/eventpoll-tree.png"></img>
  </div>
  <div style="width:49%;float:left;">
    <h6 style="color:green;">struct eventpoll 底层架构流程</h6>
    <img style="width:100%;height:215px;" src="/images/java/eventpoll.png"></img>
  </div>
  <div class="clear"></div>
</div>
]]>
    </c:sourceContent>

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>



  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.2em;">
    <c:title><![CDATA[Java NIO && Netty 的 epoll实现]]></c:title>
    <c:desc>
      <c:desc1> <![CDATA[

<h4 style="color:green;">Java NIO 根据操作系统不同， 针对 nio 中的 Selector 有不同的实现</h4>
<ul style="list-style-type:disc;margin-left:1.5em;margin-top:.625em;line-height:1.425em;">
    <li style="margin-bottom:.3em;">macosx: KQueueSelectorProvider</li>
    <li style="margin-bottom:.3em;">solaris: DevPollSelectorProvider</li>
    <li style="margin-bottom:.3em;">Linux: EPollSelectorProvider (Linux kernels >= 2.6) 或 PollSelectorProvider</li>
    <li style="margin-bottom:.3em;">windows: WindowsSelectorProvider</li>
</ul>
<div style="margin-top:.625em;">
  所以毋须特别指定，Oracle jdk 会自动选择合适的 Selector。如果想设置特定的 Selector，可以属性：
</div>
<pre style="font-size:.625em;padding:1em;background-color:beige;color:green;margin-top:.625em;border:solid 1px cadetblue;">
  -Djava.nio.channels.spi.SelectorProvider=sun.nio.ch.EPollSelectorProvider
</pre>
      ]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="Netty" id="sourceContent1" 
                     style="background-color:white;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  自4.0.16起, Netty为Linux通过JNI的方式提供了 native socket transport. 使用 native socket transport 很简单，只需将相应的类替换即可。
</div>
]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<dependency>
    <groupId>io.netty</groupId>
    <artifactId>netty-transport-native-epoll</artifactId>
    <version>${project.version}</version>
    <classifier>${os.detected.classifier}</classifer>
</dependency>

NioEventLoopGroup → EpollEventLoopGroup
NioEventLoop → EpollEventLoop
NioServerSocketChannel → EpollServerSocketChannel
NioSocketChannel → EpollSocketChannel

]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
既然如上面所说，Oracle JDK 在 Linux 已经默认使用 epoll 方式，为什么 netty 还要提供一个基于 epoll 的实现呢？这是 stackoverflow(https://stackoverflow.com/questions/23465401/why-native-epoll-support-is-introduced-in-netty) 上的一个问题。Netty 的核心开发者 Norman Maurer 这么说的：
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.625em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="color:red;padding:0;font-size:1em;line-height:1.625em;">
                     <![CDATA[
<b>Netty 的 epoll transport 使用 epoll edge-triggered 而 java 的 nio 使用 level-triggered。另外 netty epoll transport 暴露了更多的 nio 没有的配置参数，如 TCP_CORK, SO_REUSEADDR 等等</b>
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="高效的 epoll" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:blue;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
epoll 是 Linux 下，高效的多路复用技术，也是 Linux 下高性能网络服务器的关键技术。通过网络 socket 链接，把远程资源加载到本地内存中。如何来优化这个性能呢？
</div>
<div style="margin-top:.625em;">
poll 和 select 相对之前的AIO有很大的提高，但是由于需要监视着 “等待队列” 与及 “阻塞进程”，性能还是未完全释放。这个时候CPU又被别的进程给抢走，上下文切换的性能又被消耗。</div>
<div style="margin-top:.625em;">
select 要进行遍历，才能感知到那个socket来了数据，因此 select 只能一个一个遍历，来唤醒每个 socketChanel。</div>
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="epoll 原理" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:blue;font-size:1.5em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
epoll 监视多个 socket, 改进了 select 维护等待队列和阻塞进程进一步改进。把这两步动作给拆分开来。
</div>
<div style="margin-top:.325em;">
epoll_ctl 维护等待队列</div>
<div style="margin-top:.325em;">
epoll_wait 阻塞队列</div>
<div style="margin-top:.325em;">
调用 epoll_create 方法，另外会创建一个 eventpoll 对象。</div>
<div style="margin-top:.325em;">
eventpoll 维护着就绪列表，如果有 socket 来数据，就把 socket 添加到就绪列表。</div>
<div style="margin-top:.325em;">
阻塞进程收到就绪列表的回调，既可以开始 socket 数据传输工作。</div>
<div style="margin-top:.325em;">
  <img style="float:left;margin-right:1em;" src="/images/java/nio-eventpoll.webp"></img>
  <img style="float:left;border:dashed 1px orangered;" src="/images/java/eventpoll.webp"></img>
  <div class="clear"></div>
</div>
]]>
    </c:sourceContent>

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.2em;">
    <c:title><![CDATA[epoll: async-socket-server]]></c:title>
    <c:desc>
      <c:desc1> <![CDATA[
      ]]></c:desc1>
    </c:desc>
    <c:sourceContent type="" title="" id="sourceContent1" 
                     style="background-color:white;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
// Asynchronous socket server - accepting multiple clients concurrently,
// multiplexing the connections with epoll.
//
// Eli Bendersky [http://eli.thegreenplace.net]
// This code is in the public domain.
#include <assert.h>
#include <errno.h>
#include <stdbool.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <sys/epoll.h>
#include <sys/socket.h>
#include <sys/types.h>
#include <unistd.h>

#include "utils.h"

#define MAXFDS 16 * 1024

typedef enum { INITIAL_ACK, WAIT_FOR_MSG, IN_MSG } ProcessingState;

#define SENDBUF_SIZE 1024

typedef struct {
  ProcessingState state;
  uint8_t sendbuf[SENDBUF_SIZE];
  int sendbuf_end;
  int sendptr;
} peer_state_t;

// Each peer is globally identified by the file descriptor (fd) it's connected
// on. As long as the peer is connected, the fd is unique to it. When a peer
// disconnects, a new peer may connect and get the same fd. on_peer_connected
// should initialize the state properly to remove any trace of the old peer on
// the same fd.
peer_state_t global_state[MAXFDS];

// Callbacks (on_XXX functions) return this status to the main loop; the status
// instructs the loop about the next steps for the fd for which the callback was
// invoked.
// want_read=true means we want to keep monitoring this fd for reading.
// want_write=true means we want to keep monitoring this fd for writing.
// When both are false it means the fd is no longer needed and can be closed.
typedef struct {
  bool want_read;
  bool want_write;
} fd_status_t;

// These constants make creating fd_status_t values less verbose.
const fd_status_t fd_status_R = {.want_read = true, .want_write = false};
const fd_status_t fd_status_W = {.want_read = false, .want_write = true};
const fd_status_t fd_status_RW = {.want_read = true, .want_write = true};
const fd_status_t fd_status_NORW = {.want_read = false, .want_write = false};

fd_status_t on_peer_connected(int sockfd, const struct sockaddr_in* peer_addr,
                              socklen_t peer_addr_len) {
  assert(sockfd < MAXFDS);
  report_peer_connected(peer_addr, peer_addr_len);

  // Initialize state to send back a '*' to the peer immediately.
  peer_state_t* peerstate = &global_state[sockfd];
  peerstate->state = INITIAL_ACK;
  peerstate->sendbuf[0] = '*';
  peerstate->sendptr = 0;
  peerstate->sendbuf_end = 1;

  // Signal that this socket is ready for writing now.
  return fd_status_W;
}

fd_status_t on_peer_ready_recv(int sockfd) {
  assert(sockfd < MAXFDS);
  peer_state_t* peerstate = &global_state[sockfd];

  if (peerstate->state == INITIAL_ACK ||
      peerstate->sendptr < peerstate->sendbuf_end) {
    // Until the initial ACK has been sent to the peer, there's nothing we
    // want to receive. Also, wait until all data staged for sending is sent to
    // receive more data.
    return fd_status_W;
  }

  uint8_t buf[1024];
  int nbytes = recv(sockfd, buf, sizeof buf, 0);
  if (nbytes == 0) {
    // The peer disconnected.
    return fd_status_NORW;
  } else if (nbytes < 0) {
    if (errno == EAGAIN || errno == EWOULDBLOCK) {
      // The socket is not *really* ready for recv; wait until it is.
      return fd_status_R;
    } else {
      perror_die("recv");
    }
  }
  bool ready_to_send = false;
  for (int i = 0; i < nbytes; ++i) {
    switch (peerstate->state) {
    case INITIAL_ACK:
      assert(0 && "can't reach here");
      break;
    case WAIT_FOR_MSG:
      if (buf[i] == '^') {
        peerstate->state = IN_MSG;
      }
      break;
    case IN_MSG:
      if (buf[i] == '$') {
        peerstate->state = WAIT_FOR_MSG;
      } else {
        assert(peerstate->sendbuf_end < SENDBUF_SIZE);
        peerstate->sendbuf[peerstate->sendbuf_end++] = buf[i] + 1;
        ready_to_send = true;
      }
      break;
    }
  }
  // Report reading readiness iff there's nothing to send to the peer as a
  // result of the latest recv.
  return (fd_status_t){.want_read = !ready_to_send,
                       .want_write = ready_to_send};
}

fd_status_t on_peer_ready_send(int sockfd) {
  assert(sockfd < MAXFDS);
  peer_state_t* peerstate = &global_state[sockfd];

  if (peerstate->sendptr >= peerstate->sendbuf_end) {
    // Nothing to send.
    return fd_status_RW;
  }
  int sendlen = peerstate->sendbuf_end - peerstate->sendptr;
  int nsent = send(sockfd, &peerstate->sendbuf[peerstate->sendptr], sendlen, 0);
  if (nsent == -1) {
    if (errno == EAGAIN || errno == EWOULDBLOCK) {
      return fd_status_W;
    } else {
      perror_die("send");
    }
  }
  if (nsent < sendlen) {
    peerstate->sendptr += nsent;
    return fd_status_W;
  } else {
    // Everything was sent successfully; reset the send queue.
    peerstate->sendptr = 0;
    peerstate->sendbuf_end = 0;

    // Special-case state transition in if we were in INITIAL_ACK until now.
    if (peerstate->state == INITIAL_ACK) {
      peerstate->state = WAIT_FOR_MSG;
    }

    return fd_status_R;
  }
}

int main(int argc, const char** argv) {
  setvbuf(stdout, NULL, _IONBF, 0);

  int portnum = 9090;
  if (argc >= 2) {
    portnum = atoi(argv[1]);
  }
  printf("Serving on port %d\n", portnum);

  int listener_sockfd = listen_inet_socket(portnum);
  make_socket_non_blocking(listener_sockfd);

  int epollfd = epoll_create1(0);
  if (epollfd < 0) {
    perror_die("epoll_create1");
  }

  struct epoll_event accept_event;
  accept_event.data.fd = listener_sockfd;
  accept_event.events = EPOLLIN;
  if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listener_sockfd, &accept_event) < 0) {
    perror_die("epoll_ctl EPOLL_CTL_ADD");
  }

  struct epoll_event* events = calloc(MAXFDS, sizeof(struct epoll_event));
  if (events == NULL) {
    die("Unable to allocate memory for epoll_events");
  }

  while (1) {
    int nready = epoll_wait(epollfd, events, MAXFDS, -1);
    for (int i = 0; i < nready; i++) {
      if (events[i].events & EPOLLERR) {
        perror_die("epoll_wait returned EPOLLERR");
      }

      if (events[i].data.fd == listener_sockfd) {
        // The listening socket is ready; this means a new peer is connecting.

        struct sockaddr_in peer_addr;
        socklen_t peer_addr_len = sizeof(peer_addr);
        int newsockfd = accept(listener_sockfd, (struct sockaddr*)&peer_addr,
                               &peer_addr_len);
        if (newsockfd < 0) {
          if (errno == EAGAIN || errno == EWOULDBLOCK) {
            // This can happen due to the nonblocking socket mode; in this
            // case don't do anything, but print a notice (since these events
            // are extremely rare and interesting to observe...)
            printf("accept returned EAGAIN or EWOULDBLOCK\n");
          } else {
            perror_die("accept");
          }
        } else {
          make_socket_non_blocking(newsockfd);
          if (newsockfd >= MAXFDS) {
            die("socket fd (%d) >= MAXFDS (%d)", newsockfd, MAXFDS);
          }

          fd_status_t status =
              on_peer_connected(newsockfd, &peer_addr, peer_addr_len);
          struct epoll_event event = {0};
          event.data.fd = newsockfd;
          if (status.want_read) {
            event.events |= EPOLLIN;
          }
          if (status.want_write) {
            event.events |= EPOLLOUT;
          }

          if (epoll_ctl(epollfd, EPOLL_CTL_ADD, newsockfd, &event) < 0) {
            perror_die("epoll_ctl EPOLL_CTL_ADD");
          }
        }
      } else {
        // A peer socket is ready.
        if (events[i].events & EPOLLIN) {
          // Ready for reading.
          int fd = events[i].data.fd;
          fd_status_t status = on_peer_ready_recv(fd);
          struct epoll_event event = {0};
          event.data.fd = fd;
          if (status.want_read) {
            event.events |= EPOLLIN;
          }
          if (status.want_write) {
            event.events |= EPOLLOUT;
          }
          if (event.events == 0) {
            printf("socket %d closing\n", fd);
            if (epoll_ctl(epollfd, EPOLL_CTL_DEL, fd, NULL) < 0) {
              perror_die("epoll_ctl EPOLL_CTL_DEL");
            }
            close(fd);
          } else if (epoll_ctl(epollfd, EPOLL_CTL_MOD, fd, &event) < 0) {
            perror_die("epoll_ctl EPOLL_CTL_MOD");
          }
        } else if (events[i].events & EPOLLOUT) {
          // Ready for writing.
          int fd = events[i].data.fd;
          fd_status_t status = on_peer_ready_send(fd);
          struct epoll_event event = {0};
          event.data.fd = fd;

          if (status.want_read) {
            event.events |= EPOLLIN;
          }
          if (status.want_write) {
            event.events |= EPOLLOUT;
          }
          if (event.events == 0) {
            printf("socket %d closing\n", fd);
            if (epoll_ctl(epollfd, EPOLL_CTL_DEL, fd, NULL) < 0) {
              perror_die("epoll_ctl EPOLL_CTL_DEL");
            }
            close(fd);
          } else if (epoll_ctl(epollfd, EPOLL_CTL_MOD, fd, &event) < 0) {
            perror_die("epoll_ctl EPOLL_CTL_MOD");
          }
        }
      }
    }
  }

  return 0;
}
]]>
    </c:sourceContent>

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>



  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.2em;">
    <c:title><![CDATA[Linux操作系统中的零拷贝]]></c:title>
    <c:desc>
      <c:desc1> <![CDATA[

      ]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="Linux的普通I/O过程" id="sourceContent1" 
                     style="background-color:white;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<div style="margin-top:.325em;">
  <img style="float:left;border:dashed 1px orangered;margin-right:1em;padding:0 .5em;" src="/images/java/linux-io-copy.jpeg"></img>
  <img style="float:left;border:dashed 1px orangered;height:320px;padding:0 .5em;" src="/images/java/mem-mapping.jpeg"></img>
  <div class="clear"></div>
</div>

<div style="margin-top:.625em;">
  <img style="float:left;border:dashed 1px orangered;margin-right:1em;padding:0 .5em;width:427px;" src="/images/java/io-transfer.jpeg"></img>
  <img style="float:left;border:dashed 1px orangered;height: 329px;padding:0 .5em;width:427px;height:321px;" src="/images/java/zero-copy.jpeg"></img>
  <div class="clear"></div>
</div>
<div style="margin-top:.625em;">
  这是一个从磁盘文件中读取并且通过Socket写出的过程，对应的系统调用如下。
</div>
]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
read(file, tmp_buf, len);
write(socket, tmp_buf, len);
]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<ul style="margin-left:.5em;margin-top:.625em;line-height:1.425em;">
    <li style="margin-bottom:.3em;">&#x278A; 程序使用read()系统调用，系统由用户态转换为内核态，磁盘中的数据由DMA（Direct memory access）的方式读取到内核读缓冲区（kernel buffer）。DMA过程中CPU不需要参与数据的读写，而是<em style="font-size:1.125em;color:darkorchid;">DMA处理器直接将硬盘数据通过总线传输到内存中</em>。</li>
    <li style="margin-bottom:.3em;">&#x278B; 系统由内核态转为用户态，当程序要读的数据已经完全存入内核读缓冲区以后，**程序会将数据由内核读缓冲区，写入到用户缓冲区，**这个过程需要CPU参与数据的读写。</li>
    <li style="margin-bottom:.3em;">&#x278C; 程序使用write()系统调用，系统由用户态切换到内核态，数据从用户缓冲区写入到网络缓冲区（Socket Buffer），这个过程需要CPU参与数据的读写。</li>
    <li style="margin-bottom:.3em;">&#x278D; 系统由内核态切换到用户态，网络缓冲区的数据通过DMA的方式传输到网卡的驱动（存储缓冲区）中（protocol engine）</li>
</ul>

<div style="margin-top:1em;">
  可以看到，**普通的拷贝过程经历了四次内核态和用户态的切换（上下文切换），两次CPU从内存中进行数据的读写过程，**这种拷贝过程相对来说比较消耗系统资源。
</div>
]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="内存映射方式I/O" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  <img src="/images/java/mem-mapping.jpeg"></img>
</div>

]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
tmp_buf = mmap(file, len);
write(socket, tmp_buf, len);
]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<div style="margin-top:1em;">
  这是使用的系统调用方法，这种方式的I/O原理就是将**用户缓冲区（user buffer）的内存地址和内核缓冲区（kernel buffer）**的内存地址做一个映射，也就是说系统在用户态可以直接读取并操作内核空间的数据。
</div>
<ul style="margin-left:.5em;margin-top:.625em;line-height:1.425em;">
    <li style="margin-bottom:.3em;">&#x278A; mmap()系统调用首先会使用DMA的方式将磁盘数据读取到内核缓冲区，然后通过内存映射的方式，使用户缓冲区和内核读缓冲区的内存地址为同一内存地址，也就是说不需要CPU再将数据从内核读缓冲区复制到用户缓冲区。</li>
    <li style="margin-bottom:.3em;">&#x278B; 当使用write()系统调用的时候，cpu将内核缓冲区（等同于用户缓冲区）的数据直接写入到网络发送缓冲区（socket buffer），然后通过DMA的方式将数据传入到网卡驱动程序中准备发送。</li>
</ul>

<div style="margin-top:1em;">
  **可以看到这种内存映射的方式减少了CPU的读写次数，但是用户态到内核态的切换（上下文切换）依旧有四次，**同时需要注意在进行这种内存映射的时候，有可能会出现并发线程操作同一块内存区域而导致的严重的数据不一致问题，所以需要进行合理的并发编程来解决这些问题。
</div>

]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="内核空间内部传输I/O" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  <img src="/images/java/io-transfer.jpeg"></img>
</div>

]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
sendfile(socket, file, len);
]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<div style="margin-top:1em;">
  通过sendfile()系统调用，可以做到内核空间内部直接进行I/O传输。
</div>
<ul style="margin-left:.5em;margin-top:.625em;line-height:1.425em;">
    <li style="margin-bottom:.3em;">&#x278A; sendfile()系统调用也会引起用户态到内核态的切换，与内存映射方式不同的是，用户空间此时是无法看到或修改数据内容，也就是说这是一次完全意义上的数据传输过程。</li>
    <li style="margin-bottom:.3em;">&#x278B; 从磁盘读取到内存是DMA的方式，从内核读缓冲区读取到网络发送缓冲区，依旧需要CPU参与拷贝，而从网络发送缓冲区到网卡中的缓冲区依旧是DMA方式。</li>
</ul>

<div style="margin-top:1em;">
  依旧有一次CPU进行数据拷贝，两次用户态和内核态的切换操作，相比较于内存映射的方式有了很大的进步，但问题是程序不能对数据进行修改，而只是单纯地进行了一次数据的传输过程。
</div>

]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="理想状态下的零拷贝I/O" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  <img src="/images/java/zero-copy.jpeg"></img>
</div>

]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
sendfile(socket, file, len);
]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<div style="margin-top:.625em;">
  依旧是系统调用sendfile(), 可以看到，这是真正意义上的零拷贝，因为其间CPU已经不参与数据的拷贝过程，也就是说完全通过其他硬件和中断的方式来实现数据的读写过程吗，但是这样的过程需要硬件的支持才能实现。
</div>

<div style="margin-top:.625em;">
  借助于硬件上的帮助，我们是可以办到的。之前我们是把页缓存的数据拷贝到socket缓存中，实际上，我们仅仅需要把缓冲区描述符传到socket缓冲区，再把数据长度传过去，这样DMA控制器直接将页缓存中的数据打包发送到网络中就可以了。
</div>
<div style="margin-top:.625em;">
  系统调用sendfile()发起后，磁盘数据通过DMA方式读取到内核缓冲区，内核缓冲区中的数据通过DMA聚合网络缓冲区，然后一齐发送到网卡中。
</div>
<div style="margin-top:.625em;">
  可以看到在这种模式下，是没有一次CPU进行数据拷贝的，所以就做到了真正意义上的零拷贝，虽然和前一种是同一个系统调用，但是这种模式实现起来需要硬件的支持，但对于基于操作系统的用户来讲，操作系统已经屏蔽了这种差异，它会根据不同的硬件平台来实现这个系统调用。
</div>

<h4 style="color:green;margin-top:1.5em;">splice()系统调用</h4>
<div style="margin-top:.625em;">
  **splice() 系统调用和 sendfile() 非常类似，**用户应用程序必须拥有两个已经打开的文件描述符，一个用于表示输入设备，一个用于表示输出设备。与 sendfile() 不同的是，**splice() 允许任意两个文件之间互相连接，而并不只是文件到 socket 进行数据传输。**对于从一个文件描述符发送数据到 socket 这种特例来说，一直都是使用 sendfile() 这个系统调用，而 splice 一直以来就只是一种机制，它并不仅限于 sendfile() 的功能。也就是说，sendfile() 只是 splice() 的一个子集，在 Linux 2.6.23 中，sendfile() 这种机制的实现已经没有了，但是这个 API 以及相应的功能还存在，只不过 API 以及相应的功能是利用了 splice() 这种机制来实现的。
</div>
<div style="margin-top:.625em;">
  总体来讲splice()是Linux 2.6.23 内核版本中替换sendfile()系统调用的一个方法，它不仅支持文件到Socket的直接传输，也支持文件到文件的直接传输I/O，但是其底层的传输过程和sendfile()并无区别，也就是上面那两张图。
</div>
]]>
    </c:sourceContent>

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>


  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.2em;">
    <c:title><![CDATA[写时拷贝]]></c:title>
    <c:desc>
      <c:desc1> <![CDATA[

      ]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<div>
  Linux采用了写时复制的方法，以减少fork时对父进程空间进程整体复制带来的开销。
</div>

<div style="margin-top:.625em;">
  写时复制是一种采取了惰性优化方法来避免复制时的系统开销。它的前提很简单：如果有多个进程要读取它们自己的那部分资源的副本，那么复制是不必要的。每个进程只要保存一个指向这个资源的指针就可以了。只要没有进程要去修改自己的“副本”，就存在着这样的幻觉：每个进程好像独占那个资源。从而就避免了复制带来的负担。如果一个进程要修改自己的那份资源“副本”，那么就会复制那份资源，并把复制的那份提供给进程。不过其中的复制对进程来说是透明的。这个进程就可以修改复制后的资源了，同时其他的进程仍然共享那份没有修改过的资源。所以这就是名称的由来：在写入时进行复制。
</div>
<div style="margin-top:.625em;">
  写时复制的主要好处在于：如果进程从来就不需要修改资源，则不需要进行复制。惰性算法的好处就在于它们尽量推迟代价高昂的操作，直到必要的时刻才会去执行。
</div>
<div style="margin-top:.625em;">
  在使用虚拟内存的情况下，写时复制（Copy-On-Write）是以页为基础进行的。所以，只要进程不修改它全部的地址空间，那么就不必复制整个地址空间。在fork( )调用结束后，父进程和子进程都相信它们有一个自己的地址空间，但实际上它们共享父进程的原始页，接下来这些页又可以被其他的父进程或子进程共享。
</div>
<div style="margin-top:.625em;">
  写时复制在内核中的实现非常简单。与内核页相关的数据结构可以被标记为只读和写时复制。如果有进程试图修改一个页，就会产生一个缺页中断。内核处理缺页中断的方式就是对该页进行一次透明复制。这时会清除页面的COW属性，表示着它不再被共享。
</div>
<div style="margin-top:.625em;">
  现代的计算机系统结构中都在内存管理单元（MMU）提供了硬件级别的写时复制支持，所以实现是很容易的。
</div>
<div style="margin-top:.625em;">
  在调用fork( )时，写时复制是有很大优势的。因为大量的fork之后都会跟着执行exec，那么复制整个父进程地址空间中的内容到子进程的地址空间完全是在浪费时间：如果子进程立刻执行一个新的二进制可执行文件的映像，它先前的地址空间就会被交换出去。写时复制可以对这种情况进行优化。
</div>
]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>



  <c:entry style="margin-top:1em;color:rgb(175, 0, 190);font-size:2.2em;">
    <c:title><![CDATA[Linux中最大进程数和最大文件数]]></c:title>
    <c:desc>
      <c:desc1> <![CDATA[
<div>
  Linux系统中可以设置关于资源的使用限制，比如：进程数量，文件句柄数，连接数等等。在日常的工作中应该遇到过：
</div>
<pre style="margin-top:.325em;">
$ -bash: fork: retry: Resource temporarily unavailable 或者
$ too many open files
</pre>
<div style="margin-top:.625em;">
  这些类似的操作错误，前者是由于当前用户的进程数超出限制，后者由于当前用户的文件打开数超出限制。
</div>
      ]]></c:desc1>
    </c:desc>
    <c:sourceContent type="" title="通过参数-a查看当前用户的所有限制情况" id="sourceContent1" 
                     style="background-color:white;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
$ ulimit -a
core file size          (blocks, -c) unlimited
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 0
file size               (blocks, -f) unlimited
pending signals                 (-i) 127510
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 32768
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 4096         # 系统限制某用户下最多可以运行多少进程
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited

$ ulimit -u 8192 设置当前用户的最大进程数为8192，只在当前shell中生效
$ ulimit -n 8192 设置当前用户的最大文件打开数8192，只在当前shell中生效

#### root 账号下 ulimit -u 出现的 max user processes 的值默认是 /proc/sys/kernel/threads-max 的值 / 2，即系统线程数的一半
#### 普通账号下 ulimit -u 出现的 max user processes 的值默认是 /etc/security/limits.d/20-nproc.conf (centos6 是 90-nproc.conf) 文件中的
# Default limit for number of user's processes to prevent
# accidental fork bombs.
# See rhbz #432903 for reasoning.
*          soft    nproc     8192
root       soft    nproc     unlimited
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="如何修改这些个值" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<ul style="margin-left:.5em;margin-top:.0em;line-height:1.425em;">
    <li style="margin-bottom:.3em;">&#x278A; 将命令写至 profile 和 bashrc 中，相当于在登陆时自动动态修改限制。</li>
    <li style="margin-bottom:.3em;">&#x278B; 在 /etc/security/limits.conf 中添加记录（需重启生效，并且在 /etc/pam.d/ 中的 seesion 有使用到 limit 模块）。</li>
    <li style="margin-bottom:.3em;">&#x278C; 修改 /etc/security/limits.conf 对普通用户 max user processes 值是不生效的，需要修改 /etc/security/limits.d/20-nproc.conf (centos6 是 90-nproc.conf)  文件中的值。</li>
    <li style="margin-bottom:.3em;">&#x278D; 如果使用 * 号让全局用户生效是受文件 /etc/security/limits.d/20-nproc.conf 中 nproc 值大小制约的，而如果仅仅是针对某个用户，那么就不受该文件 nproc 值大小的影响。</li>
</ul>

]]>
    </c:sourceContent>
    <c:sourceContent type="" title="/etc/security/limits.conf" id="sourceContent1" 
                     style="background-color:white;margin-top:.625em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
#Each line describes a limit for a user in the form:
#
#<domain>        <type>  <item>  <value>
#
#Where:
#<domain> can be:
#        - a user name
#        - a group name, with @group syntax
#        - the wildcard *, for default entry
#        - the wildcard %, can be also used with %group syntax,
#           for maxlogin limit
#
#<type> can have the two values:
#        - "soft" for enforcing the soft limits
#        - "hard" for enforcing hard limits
#
#<item> can be one of the following:
#        - core - limits the core file size (KB)
#        - data - max data size (KB)
#        - fsize - maximum filesize (KB)
#        - memlock - max locked-in-memory address space (KB)
#        - nofile - max number of open file descriptors
#        - rss - max resident set size (KB)
#        - stack - max stack size (KB)
#        - cpu - max CPU time (MIN)
#        - nproc - max number of processes
#        - as - address space limit (KB)
#        - maxlogins - max number of logins for this user
#        - maxsyslogins - max number of logins on the system
#        - priority - the priority to run user process with
#        - locks - max number of file locks the user can hold
#        - sigpending - max number of pending signals
#        - msgqueue - max memory used by POSIX message queues (bytes)
#        - nice - max nice priority allowed to raise to values: [-20, 19]
#        - rtprio - max realtime priority
#
#<domain>      <type>  <item>         <value>
#

#*               soft    core            0
#*               hard    rss             10000
#@student        hard    nproc           20
#@faculty        soft    nproc           20
#@faculty        hard    nproc           50
#ftp             hard    nproc           0
#@student        -       maxlogins       4

# End of file
* soft nofile 32768
* hard nofile 65536
#use for oracle
postgres soft nproc 2047
postgres hard nproc 16384
postgres soft nofile 2048
postgres hard nofile 65536
postgres soft stack 10240
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.625em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[

<ul style="margin-left:.5em;margin-top:.625em;line-height:1.425em;">
    <li style="margin-bottom:.3em;">&#x278A; domain 部分：用来表示用户名或者组名。@后接用户名或者组名表示用户名或者组名；*表示所有用户。</li>
    <li style="margin-bottom:.3em;">&#x278B; type 部分：表示资源层面，有两种值soft和hard。soft表示在软件层面，hard表示在硬件层面。</li>
    <li style="margin-bottom:.3em;">&#x278C; item 部分：表示可以限制的资源类型。core限制核心的文件大小；locks表示用户可以持有的最大文件锁数量；nproc表示最大的进程数；nofile表示最大的打开文件句柄数。</li>
    <li style="margin-bottom:.3em;">&#x278D; value部分：限制的资源数量的值。</li>
</ul>

<div style="margin-top:.625em;">
  如上面的配置： <br />
  student hard nproc 20，表示student用户在硬件上最大的进程数只有20，<br />
  * soft nofile 32768表示软件上所有用户的最大文件打开数是32768。<br />
  <b style="color:blue;">noproc 是代表最大进程数。<br />
  nofile 是代表最大文件打开数。</b>
</div>
<div style="margin-top:.625em;">
  只修改了这个 /etc/security/limits.conf 文件对普通用户是不生效的，还需要修改 /etc/security/limits.d/90-nproc.conf 配置文件，可以先查看下这个文件的内容：
</div>
]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:.325em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
# Default limit for number of user's processes to prevent
# accidental fork bombs.
# See rhbz #432903 for reasoning.

*          soft    nproc     8192
root       soft    nproc     unlimited
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  从注释中可以看出，是针对用户进程的默认限制。从配置中可以看出 root 用户不做进程数量的限制，其他用户的最大进程数为 8192。
</div>
]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="系统总限制" id="sourceContent1" 
                     style="background-color:white;margin-top:1em;"
                     titleStyle="padding-left:0;color:black;font-size:1.2em;background-color:white;"
                     bodyStyle="background-color:white;padding:0;font-size:1em;line-height:1.625em;font-family:monospace;">
                     <![CDATA[
<div>
  其实上面的 max user processes 65535 的值也只是表象，普通用户最大进程数无法达到 65535 ，因为用户的 max user processes 的值，最后是受全局的 kernel.pid_max 的值限制的。也就是说 kernel.pid_max=1024，那么你用户的 max user processes 的值是127426，用户能打开的最大进程数还是1024.
</div>

<h4 style="color:green;margin-top:1em;">查看全局的 pid_max 方法</h4>
<div style="margin-top:.325em;">
  $ cat /proc/sys/kernel/pid_max 或 $ sysctl kernel.pid_max
</div>

<h4 style="color:green;margin-top:1em;">修改这个值方法</h4>
<div style="margin-top:.325em;">
  $ echo 65535 > /proc/sys/kernel/pid_max
</div>
<div style="margin-top:.325em;">
  以上都操作完成后，才算是正确修改了 max user processes 的值
</div>

<h4 style="color:green;margin-top:1em;">永久生效方法</h4>
<pre style="margin-top:.325em;">
$ vim /etc/sysctl.conf
> kernel.pid_max = 65535
</pre>

<div style="margin-top:.325em;">
  然后重启机器。
</div>
]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>


</c:component>
