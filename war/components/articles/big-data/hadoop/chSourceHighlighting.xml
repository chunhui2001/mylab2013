<?xml version="1.0" encoding="utf-8"?>
<c:component type="chSourceHighlighting" componentId="chSourceHighlighting_1" xmlns:c="http://com.snnmo.website">
  <c:entry>
    <c:title></c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="分布式系统和 Hadoop" id="sourceContent1">
      <![CDATA[
    <p style="margin-bottom:1em;">对于一个有 4 个 I/O 通道的高端机，即使每个通道的吞吐量各为 100MB/s，读取 4TB 的数据集也需要 3 个小时。
    而利用 Hadoop，同样的数据集会被划分为较小的块 (通常为 64MB)，通过 Hadoop 分布式文件系统 (HDFS) 分布在集群内多台机器上。
    使用适度的复制，集群可以并行读取数据，进而提供很高的吞吐量。这样一组通用机器比一台高端服务器更加便宜。</p>
    <br />
    <p>Hadoop 强调把代码向数据迁移，即： 让数据不动，而将可执行代码发送到数据所在的机器上去。
    数据被拆分后在集群中分布，并且尽可能让一段数据的计算发生在同一台机器上，既这段数据驻留的地方。
    这里所说的代码指的就是 MapReduce 程序。</p>
    <br />
    <p>Hadoop 使用 MapReduce 做为其数据处理框架。MapReduce 程序的核心是 Map 和 Reduce 操作，但实际上这个框架还包含其它操作，如: 
    数据分割 (data splitting)、洗牌 (shuffling)、分组 (Partitioning) 和 合并 (Combining)。</p>
    ]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="比较 SQL 数据库和 Hadoop" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
    <p><span style="font-weight:bold">用横向扩展替代纵向扩展：</span>前者加硬件后者加机器</p>
    <p><span style="font-weight:bold">用键值对替代关系表：</span> Hadoop 的数据来源可以使任意形式，但最终会转换为键值对以供处理。</p>
    <p><span style="font-weight:bold">用函数式编程 (MapReduce) 替代声明式查询 (SQL)：</span>对于习惯 SQL 范式的人，用 MapReduce 来思考是一个挑战。</p>
    <p><span style="font-weight:bold">离线处理代替在线处理：</span>Hadoop 适合一次写入、多次读取的数据存储需求。类似于 SQL 世界中的数据仓库。</p>
    ]]>
    </c:sourceContent>


    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>扩展一个单词统计程序</c:title>
    <c:desc>
      <c:desc1> <![CDATA[统计一组文档中每个单词出现的次数]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="单词统计" id="sourceContent1" style="border-bottom:none;">
      <![CDATA[
    Do as I say, not as I do.
    ]]>
    </c:sourceContent>


    <c:sourceContent type="" title="" id="sourceContent1" style="border-top:none;border-bottom:none;">
      <![CDATA[// 实现该程序的伪代码如下;
define wordCount as Multiset;
for each document in documentSet {
  T = tokenize(document);
  for each token in T {
    wordCount[toke]++;
  }
}
display(wordCount);]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;border-bottom:none;">
      <![CDATA[该程序只适合处理少量文档，一旦文档数量激增，它就不能胜任了：<b>使用单台计算机反复遍历所有文档将会非常费时。</b><br /><br />
      重写程序让工作可以分布在多台机器上，每台计算机处理这些文档的不同部分。当所有的机器都完成时，第二个处理阶段将合并这结果。]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-top:none;border-bottom:none;">
      <![CDATA[
// 第一阶段要分布到多台机器上去的伪代码为: 
define wordCount as Multiset;
for each document in documentSet {
  T = tokenize(document);
  for each token in T {
    wordCount[toke]++;
  }
}
sendToSecondPhase(wordCount);

// 第二阶段的伪代码为: 
define totalWordCount as Multiset;
for each wordCount received from firstPhase {
  multisetAdd (totalWordCount, wordCount);
}]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;">
      <![CDATA[为了使该程序工作在一个分布式计算机集群上，需要添加以下功能：<br />
     <p style="font-weight:bold;"> 1、存储文件到许多台计算机上 (第一阶段)。<br />
      2、编写一个基于磁盘的散列表，使得处理不受内存容量限制。 <br />
      3、划分来自第一阶段的中间数据 (即 wordCount)。 <br />
      4、洗牌到这些分区到第二阶段中合适的计算机上。</p>]]>
    </c:sourceContent>


    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>MapReduce</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <!--c:sourceContent type="html" title="管道和消息队列" id="sourceContent1">
      <![CDATA[
    <p style="margin-bottom:0em;"><b>管道</b>和<b>消息队列</b>等数据处理模型可专用于数据处理应用的方方面面 (Unix pipes 就是一种常见的管道)。</p>
    <p style="margin-bottom:0em;"><b>管道</b>有助于<b>进程原语</b>的重用，已有模块的简单链接即可组成一个新的模块。</p>
    <p style="margin-bottom:0em;"><b>消息队列</b>有助于<b>进程原语</b>的同步，程序员将数据处理任务以生产者或消费者的形式编写为进程原语，由系统来管理它们何时执行。</p>
    
    
    <p style="margin-top:1em;"><b>MapReduce</b> 也是一个数据处理模型，它的最大优点是容易扩展到多个计算节点上处理数据。</p>
    <p style="margin-top:0em;">在 <b>MapReduce</b> 模型中，数据处理原语被称为 mapper 和 reducer。
    分解一个数据处理应用为 mapper 和 reducer 有时是繁琐的，但是一旦以 MapReduce 的形式写好一个应用程序，
    仅需修改配置就可以将它扩展到集群中几百、几千甚至上万台机器上运行。</p>
    ]]>
    </c:sourceContent-->
    <c:sourceContent type="html" title="MapReduce" id="sourceContent1">
      <![CDATA[
    <p><b>MapReduce</b> 算法将查询操作和数据集都分解为组件&mdash;这就是<b>映射 (Map)</b>。
    在查询中被映射的组件可以被同时处理 (即<b>规约:Reduce</b> ) 从而快速地返回结果。不幸的是 <b>MapReduce</b>  是一个在概念和实现上都很复杂的想法！</p>
    <br />
    <p><b>管道</b>和<b>消息队列</b>等数据处理模型，用于数据处理应用的方方面面 (Unix pipes 就是一种常见的管道)。
        <b>管道</b>有助于进程原语的重用，已有模块的简单链接即可组成一个新的模块；<b>消息队列</b>则有助于进程原语的同步。
        程序员将数据处理任务编写为进程原语，由系统来管理它们何时执行。</p>
    <br />
    <p>MapReduce 做为一个数据处理模型，它的最大优点是容易扩展到多个计算节点上处理数据。
      在 <b>MapReduce</b> 模型中，数据处理原语被称为 <b>mapper</b> 和 <b>reducer</b>。分解一个数据处理应用为 mapper 和 reducer 有时是繁琐的。
      但是一旦以 MapReduce 的形式写好了一个应用程序，仅需修改配置就可以将它扩展到集群上运行。</p>
    <br />
    <p>MapReduce 程序的执行分为两个主要阶段，即：mapping 和 reducing。每个阶段均定义成一个数据处理函数，分别称为 mapper 和 reducer。
       <b>在 mapping 阶段，MapReduce 获取输入数据并将数据单元装入 mapper。在 reducing 阶段，reducer 处理来自 mapper 的所有输出，并给出最终结果。</b>
       <b style="color:green;">mapper 意味着将输入进行过滤转换 (通常是转换成键值对)，使 reducer 可以完成聚合。</b></p>
    <br />
    <p>MapReduce 使用<b>列表</b>和<b>键/值</b>对作为其主要的数据原语。</p>
    <table style="margin-top:.5em;">
      <thead>
        <tr>
          <th style="padding:.2em .3em;border-right:solid 1px rgb(148, 138, 138);"></th>
          <th style="padding:.2em .3em;font-weight:bold;border-right:solid 1px rgb(148, 138, 138);border-top:solid 1px rgb(148, 138, 138);background-color:rgb(226, 220, 220);">输入</th>
          <th style="padding:.2em .3em;font-weight:bold;border-top:solid 1px rgb(148, 138, 138);background-color:rgb(226, 220, 220);">输出</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align:right;padding:.2em .3em;border-right:solid 1px rgb(148, 138, 138);border-top:solid 1px rgb(148, 138, 138);">map</td>
          <td style="padding:.2em .3em;border-right:solid 1px rgb(148, 138, 138);border-top:solid 1px rgb(148, 138, 138);">&lt;k1, v1&gt;</td>
          <td style="padding:.2em .3em;border-top:solid 1px rgb(148, 138, 138);">list(&lt;k2, v2&gt;)</td>
        </tr>
        <tr>
          <td style="text-align:right;padding:0 .3em;border-right:solid 1px rgb(148, 138, 138);border-top:solid 1px rgb(148, 138, 138);border-bottom:solid 1px rgb(148, 138, 138);">reduce</td>
          <td style="padding:.2em .3em;border-right:solid 1px rgb(148, 138, 138);border-top:solid 1px rgb(148, 138, 138);border-bottom:solid 1px rgb(148, 138, 138);">&lt;k2, list(v2)&gt;</td>
          <td style="padding:.2em .3em;border-top:solid 1px rgb(148, 138, 138);border-bottom:solid 1px rgb(148, 138, 138);">list(&lt;k3, v3&gt;)</td>
        </tr>
      </tbody>
    </table>
    <br />
    <p><b>在 MapReduce 中编写程序就是定制化 mapper 和 reducer 的过程，其完整的数据流如下：</b></p>
    <ul style="list-style:decimal;margin-left:1.3em;margin-top:.5em;">
      <li>应用的输入必须组织为一个键/值对列表 <b>list(&lt;k1, v1&gt;)</b>。用于处理文件的输入格式通常为 <b>list(&lt;fileName, fileContent&gt;)</b>。
         用于处理类似日志文件的大文件的输入格式为 <b>list&lt;lineNumber, lineContent&gt;</b>。<br /><br /></li>
      <li>含有键值对的列表被拆分，进而通过调用 mapper 的 map 函数对每个单独的键值对 <b>&lt;k1, v1&gt;</b> 进行处理。
      mapper 转换每个 <b>&lt;k1, v1&gt;</b> 对并将之放入 <b>&lt;k2, v2&gt;</b> 对的列表中。
      <br /> <br />
      对于下面的单词统计程序，<b>&lt;String fileName, String fileContent&gt;</b> 被输入 mapper，而其中的 fileName 可以被忽略。
      mapper 可以输出一个 <b>&lt;String word, Integer count&gt;</b> 的列表。<br /><br />
      </li>
      <li>所有 mapper 的输出被聚合到一个包含 <b>&lt;k2, v2&gt;</b> 的巨大列表中。
          所有共享相同 k2 的对被组织在一起形成一个新的键值对 <b>&lt;k2, list(v2)&gt; </b>。
          
          框架让 reducer 来分别处理没一个被聚合起来的 <b>&lt;k2, list(v2)&gt; </b>。
          <br /> <br />
      对于下面的单词统计程序，一个文档的 map 输出的列表中可能出现三次 <b>&lt;'foo', 1&gt;</b>，而另一个文档的 map 输出可能出现两次 <b>&lt;'foo', 1&gt;</b>。
      reducer 看到的聚合对为 <b>&lt;'foo', list(1,1,1,1,1)&gt;</b>，此时 reducer 的输出为 <b>&lt;'foo', 5&gt;</b>。
          <br /> <br />
          每一个 reducer 负责不同的单词，MapReduce 框架自动搜集所有的 <b>&lt;k3, v3&gt;</b> 对，并将之写入文件。
      
      <br /><br /></li>
    </ul>
    ]]>
    </c:sourceContent>



    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>动手扩展一个单词统计程序</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="" title="Do as I say, not as I do." id="sourceContent1">
      <![CDATA[define wordCount as Multiset;
for each document in documentSet {
  T = tokenize (document);
  for each token in T {
    wordCount[token]++;
  }
}
display(wordCount);]]>
    </c:sourceContent>

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>基于 MapReduce 重写单词统计程序</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="" title="单词统计中 map 和 reduce 函数的伪代码" id="sourceContent1">
      <![CDATA[
/* 因为 map 和 reduce 的输出都是列表，
   所以可以使用 Hadoop 中的 emit 函数生成列表中的元素 */ 
map (String fileName, String document) {
  List<String> T = tokenize (document);
      
  for each token in T {
    emit ((String) token, (Integer) 1);
  }
}
    
reduce (String token, List<Integer> values) {
  Integer sum = 0;      
  for each value in values {
    sum = sum + value;
  }      
  emit ((String) token, (Integer) sum);
}]]>
    </c:sourceContent>



    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>用 Hadoop 统计单词</c:title>
    <c:desc>
      <c:desc1> <![CDATA[从网站 <a href="http://hadoop.apache.org/core/releases.html">http://hadoop.apache.org/core/releases.html</a> 上下载最新的稳定版本]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="在 Mac 上安装 Hadoop" id="sourceContent1">
      <![CDATA[打开 Hadoop 发布包 &rarr; 编辑脚本 conf/hadoop-env.sh &rarr; 将 JAVA_HOME 设置为 Java 安装目录：<br />
      <b>export JAVA_HOME=/Library/Java/Home</b> <br /><br />
      
      不加任何参数的运行 <b>bin/hadoop</b>，显示 Hadoop 的用法文档: <br />
      <img style="margin-top:.5em;width:100%;" src="//c1.staticflickr.com/3/2933/13997007728_8bdab253b0_c.jpg" />
      
      <br />
      <br />
      运行 hadoop 示例程序：<b>bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount input/ output/</b> 
      <br />
      <br />
      如果我们运行作业时仅仅是想看到 mapper 的输出，可以用选项 -D mapred.reduce.tasks=0 将 reducer 的数目设置为 0：<b>
      <br />
      bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount -D mapred.reduce.tasks=0 input/ output/</b> 
      <br />
      <br />
      将提示 wordcount 程序的参数列表：<br />
      <b>wordcount [-m &lt;maps&gt;] [-r &lt;reduces&gt;] &lt;input&gt; &lt;output&gt;</b><br /><br />
      
      <b>&lt;input&gt;</b> 指的是所需分析的文本文档的输入目录。(可以从<a href="http://www.gpoaccess.gov/sou/">这里</a>下载所需的文本文件)<br />
      <b>&lt;output&gt;</b> 指的是程序填充结果的输出目录。<br /><br />
      
      程序运行完毕之后，在 output 文件夹下将看到所有单词的统计结果...
]]>
    </c:sourceContent>



    <c:comment>
      <c:comment1>
        <![CDATA[Windows 也可以支持开发模式，但你需要在节点上安装 <a href="http://www-cygwin.com/">cygwin</a> 来支持 shell 和 Unix 脚本]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>修改 wordcount 源码</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="适应 Hadoop API 的改变" id="sourceContent1" style="border-bottom:none;">
      <![CDATA[<ul style="list-style:decimal;margin-left:1.5em;font-weight:bold;">
        <li>在新 API 中 org.apache.hadoop.mapred 下所有的类都被弃用，取而代之的是 org.apache.hadoop.mapreduce。</li>
        <li>在新 API 中引入了上下文对象 context，context 替换了 map() 和 reduce() 方法中使用的 OutputCollector 和 Reporter 对象。</li>
        <li>在新 API 中使用 context.write() 输出键/值对，而不是 OutputCollector.collect()。</li>
        <li>新 API 统一了应用代码和 MapReduce 框架之间的通信，并固定了 Mapper 和 Reducer 的 API，使得添加新功能时不会改变基本方法签名。</li>
        <li>在新 API 中添加新的功能仅仅是在 context 对象上添加新方法，所以原有程序不会感觉到新方法的存在，它们任然可以在新版本中继续编译和运行。</li>
        <li>在新 API 中 map() 和 reduce() 方法分别被包含在新的抽象类 Mapper 和 Reducer 中，新的抽象类也替换了 MapReduceBase 类，使之被启用。</li>
        <li>在新 API 中 map() 和 reduce() 方法可以抛出 InterruptedException 而非单一的 IOException。</li>
        <li>reduce() 方法用 Iterable 替换 Iterator，这样更容易使用 Java 的 foreach 实现迭代。</li>
      </ul>
      <br />]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<b>原始 API 中 map() 和 reduce() 的签名：</b>]]>
    </c:sourceContent>

    <c:sourceContent type="" title="" id="sourceContent1" style="border-top:none;border-bottom:none;">
      <![CDATA[public static class MapClass extends MapReduceBase 
                             implements Mapper<K1, V1, K2, V2> {
    public void map(K1 key, V1 value
                    , OutputCollector<K2, V2> output, Reporter reporter) 
                    throws IOException { }
}

/*
    每一个 map() 方法的调用分别被赋予一个类型为 K1 和 V1 的键/值对。
    这个键/值对由 mapper 生成，并通过 OutputCollector 对象的 collect() 方法输出。
*/





public static class Reduce extends MapReduceBase 
                           implements Reducer<K2, V2, K3, V3> {
    public void reduce(K2 key, Iterator<V2> values
                       , OutputCollector<K3, V3> output, Reporter reporter) 
                       throws IOException { }
}

/*
    reduce() 方法的每次调用均被赋予 K2/V2 类型的一组 键/值，
    它必须与 Mapper 中使用的 K2 和 V2 类型相同。reduce() 方法可能会循环遍历 V2 类型的所有值。
    reduce() 还使用 OutputCollector 类来输出键/值，它们的类型为 K3/V3。
*/]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<b>新 API 中 map() 和 reduce() 的签名：</b>]]>
    </c:sourceContent>

    <c:sourceContent type="" title="" id="sourceContent1" style="border-top:none;border-bottom:none;">
      <![CDATA[public static class MapClass extends 
                             Mapper<K1, V1, K2, V2> {
    public void map(K1 key, V1 value
                    , Context context) 
                    throws IOException, InterruptedException { }
}


public static class Reduce extends 
                           Reducer<K2, V2, K3, V3> {
    public void reduce(K2 key, Iterable<V2> values
                    , Context context) 
                    throws IOException, InterruptedException { }
}]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;">
      <![CDATA[<b>还需要改变 driver 中的一些内容来支持新的 API：</b><br /><br />
      <ul style="list-style:decimal;margin-left:1.5em;font-weight:bold;">
        <li>在新 API 中 JobConf 和 JobClient 被替换了，它们的功能已经被放入 Configuration 类和一个新的 Job 类中。</li>
        <li>Configuration 类纯粹是为了配置作业而设，而 Job 类负责定义和控制一个作业的执行。</li>
        <li>诸如 setOutputKeyClass() 和 setOutputValueClass() 等方法被从 JobConf 转移到了 Job 类。</li>
        <li>作业的构造和提交执行放在 Job 中。</li>
      </ul>
      <br /><br />
      <b>原本需要使用 JobConf 来构造一个作业：</b><br />
      <b style="color:green;">
        JobConf job = new JobConf(conf, MyJob.class); <br />
        job.setJobName("MyJob");
      </b>
      <br />
      <br />
      <b>现在可以通过 Job 类完成：</b><br />
      <b style="color:green;">
        Job job = new Job(conf, "MyJob"); <br />
        job.setJarByClass(MyJob.class);
      </b>
      <br />
      <br />
      <b>以前通过 JobClient 提交作业去执行：</b><br />
      <b style="color:green;">
        JobClient.runJob(job);
      </b>
      <br />
      <br />
      <b>现在同样通过 Job 类完成：</b><br />
      <b style="color:green;">
        System.exit(job.waitForCompletion(true) ? 0 : 1);
      </b>
      <br />
      <br />
      ]]>
    </c:sourceContent>

    <c:sourceContent expand="false" type="" title="建立 WordCount.java 源码副本" id="sourceContent1">
      <![CDATA[mkdir playground
mkdir playground/src
mkdir playground/classes
cp hadoop-2.2.0-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java
        playground/src/WordCount.java

// 在 Hadoop 框架中编译和执行该副本
javac -classpath hadoop-*-core.jar -d playground/classes
        playground/src/WordCount.java
或 
javac -classpath $HADOOP_HOME/share/hadoop/common/hadoop-common-2.2.0.jar:
                 $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.2.0.jar:
                 $HADOOP_HOME/share/hadoop/common/lib/commons-cli-1.2.jar:
                 $HADOOP_HOME/share/hadoop/common/lib/hadoop-annotations-2.4.0.jar
      -d wordcount_classes WordCount.java
        
jar -cvf playground/wordcount.jar -C playground/classes/ .

// 运行该副本 (如 output 目录存在应先将其删除)
bin/hadoop jar playground/wordcount.jar org.apache.hadoop.examples.WordCount input output]]>
    </c:sourceContent>

    <c:sourceContent expand="false" type="" title="修改 WordCount.java 源码" id="sourceContent1">
      <![CDATA[package org.apache.hadoop.examples;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class WordCount {

  public static class TokenizerMapper 
       extends Mapper<Object, Text, Text, IntWritable>{
    
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
      
    public void map(Object key, Text value, Context context) 
                                  throws IOException, InterruptedException {
      // 使用空格进行分词 
      //StringTokenizer itr = new StringTokenizer(value.toString());                           
      
      // 根据标点符号分词 
      StringTokenizer itr = new StringTokenizer(value.toString(), " \t\n\r\f,.:;?![]'");   
      
      while (itr.hasMoreTokens()) {      
        // 在 Hadoop 中, 特殊的 Text 类取代了 String。
        // 把 Token 放入 Text 对象中
        // 在将 String 转成 Text 之前，先将所有单词转成小写.
        // word.set(itr.nextToken());              
        word.set(itr.nextToken().toLowerCase());  
                                                  
        context.write(word, one);
      }
    }
  }
  
  public static class IntSumReducer 
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) 
                                  throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      
      // 仅统计数量大于 4 的单词
      // 输出每个 Token 的统计结果
      if (sum > 4) context.write(key, result);           
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length != 2) {
      System.err.println("Usage: wordcount <in> <out>");
      System.exit(2);
    }
    Job job = new Job(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}]]>
    </c:sourceContent>



    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>Hadoop 的构造模块</c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[在一个全配置的集群上，运行 Hadoop 意味着在网络分布的不同服务器上运行一组守护进程。
      这些守护进程有特殊的角色，一些仅存在于单个服务器上，一些运行在多个服务器上。]]>
      </c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="Hadoop 的守护进程" id="sourceContent1">
      <![CDATA[<p style="font-weight:bold;">
      NameNode (名字节点) <br />
      DataNode (数据节点) <br />
      Secondary NameNode (次名字节点) <br />
      JobTracker (作业跟踪节点) <br />
      TaskTracker (任务跟踪节点)</p> ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="NameNode" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>Hadoop 在<b>分布式计算</b>与<b>分布式存储 (HDFS)</b>中均采用了<b>主/从 (master/slave)</b>结构。<br />
      <b>2、</b>NameNode 位于 HDFS 的主端，它指导从端的 DataNode 执行底层的 I/O 任务。<br />
      <b>3、</b>NameNode 是 HDFS 的书记员，它跟踪文件如何被分割成文件块，这些块又被哪些节点存储，以及分布式文件系统的整体运行状态是否正常。<br />
      <b>4、</b>运行 NameNode 会消耗大量内存和 I/O 资源，所以驻留 NameNode 的服务器通常不会存储用户数据或者执行 MapReduce 程序的计算任务。<br />
      <b>5、</b>除了 NameNode，其它任何守护进程驻留的节点发生故障或失效均不会影响 Hadoop 集群的整体运行。]]>
    </c:sourceContent>

    <c:sourceContent type="html" expand="false" title="DataNode" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>Hadoop 集群上的所有节点都会驻留一个 DataNode 守护进程，来执行<b>HDFS</b>的繁重工作 (将 HDFS 数据块读取或写入到本地文件系统的实际文件中)。<br />
      <b>2、</b>当希望对 HDFS 文件进行读写时，文件被分割为多个块，由 NameNode 告知客户端每个数据块驻留在哪个 DataNode 节点。<br />
      <b>3、</b>客户端直接与 DataNode 守护进程通信，来处理与数据块相对应的本地文件。而后，DataNode 会与其它 DataNode 节点通信，复制这些数据块以实现冗余。<br />
      <b>4、</b>下图是 NameNode 和 DataNode 在 HDFS 中的交互。NameNode 跟踪文件的元数据--描述系统中所包含的文件以及每个文件如何被分割为数据块。
      DataNode 提供数据块的备份存储，并持续不断的向 NameNode 报告，以保持元数据为最新状态。<br />
      <img src="//c1.staticflickr.com/3/2939/14227977545_b73c8e45ca_z.jpg" style="width:100%;" /><br />
      <b style="color:green;">在上图中有两个数据文件，一个位于目录 /user/chuck/data1，另一个位于 /user/james/data2。
      data1 有 1、2、3 三个数据块，data2 有 4、5 两个数据块。
      其中每个数据块有三个副本，这确保了任何一个 DataNode 崩溃，数据文件仍然可以访问。
      DataNode 会不断地更新 NameNode，为之提供本地修改的相关信息，同时接收相关指令以进行创建、移动、或删除本地磁盘上的数据块。</b>]]>
    </c:sourceContent>

    <c:sourceContent type="html" expand="false" title="Secondary NameNode (SNN)" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>SNN 是一个用于监测 HDFS 集群状态的辅助守护进程。它与 NameNode 通信，根据集群所配置的时间间隔获取 HDFS 元数据的快照。<br />
      <b>2、</b>NameNode 是 Hadoop 集群的单一故障点，而 SNN 的数据快照有助于减少停机的时间并降低数据丢失的风险。<br />
      <b>3、</b>NameNode 的失效处理需要人工的干预，即手动地重新配置集群，将 SNN 用作主要的 NameNode。<br />
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" expand="false" title="JobTracker" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>JobTracker 守护进程是应用程序和 Hadoop 之间的纽带。
      一旦提交代码到集群上，JobTracker 就会确定执行计划，包括决定处理哪些文件、为不同的任务分配节点以及监控所有任务的运行。<br />
      <b>2、</b>如果任务失败，JobTracker 将自动重启任务，同时受到预定义的重试次数限制。<br />
      <b>3、</b>每个 Hadoop 集群只有一个 JobTracker 守护进程，它通常运行在服务器集群的主节点上。
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" expand="false" title="TaskTracker" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>与存储的守护进程一样，计算的守护进程也遵循主/从架构： JobTracker 作为主节点，监测 MapReduce 作业的整个执行过程。<br />
      <b>2、</b>TaskTracker 管理各个任务在每个从节点上的执行情况，并负责执行由 JobTracker 分配的单项任务。<br />
      <b>3、</b>每个从节点上仅有一个 TaskTracker，但每个 TaskTracker 可以生成多个 JVM 来并行地处理许多 map 或 reduce 任务。<br />
      <b>3、</b>TaskTracker 的一个职责是持续不断地与 JobTracker 通信。
            如果 JobTracker 在指定的时间内没有收到来自 TaskTracker 的 "心跳"，它会假定 TaskTracker 已经崩溃了，进而重新提交相应的任务到集群中的其它节点中。<br />
            <br />
      <b>Hadoop 集群的拓扑结构：</b><br />
      <img src="//c1.staticflickr.com/3/2915/14248462943_c30a94a4af.jpg" style="width:100%;" /><br />
      <b>这个拓扑结构遵从主/从架构，其中 NameNode 和 JobTracker 为主端，DataNode 和 TaskTracker 为从端。</b>
      ]]>
    </c:sourceContent>


    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>组建一个完整的 Hadoop 集群</c:title>
    <c:desc>
      <c:desc1> <![CDATA[1、首先建立主节点以及节点之间的控制通道，即 Secure Shell (SSH) 通道。]]></c:desc1>
      <c:desc1>
        <![CDATA[2、SSH 采用标准的公钥加密来生成一对用户验证密钥--一个公钥、一个私钥。
      公钥存储在集群的每个节点上，私钥则由主节点在试图访问远端节点时发送过来。结合公钥与私钥，目标机可以对这次访问进行验证。]]>
      </c:desc1>
      <c:desc1> <![CDATA[3、所有节点的登陆账号应该有相同的用户名 (例如使用：hadoop-user)]]></c:desc1>
      <c:desc1> <![CDATA[4、其次建立单机或者伪分布模式或者标准的集群安装模式 (全分布模式)]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="验证 SSH 安装" id="sourceContent1">
      <![CDATA[<b>$which ssh<br />
      $which sshd<br />
      $which ssh-keygen </b><br />
      
      <br />
      如果收到如下消息，说明没有安装：<br />
      <b>$no ssh in (/user/bin:/bin:/user/sbin .... </b><br />
      <br />
      可以通过 Linux 安装包管理器安装 OpenSSH (www.openssh.com) 或者直接下载其源代码。
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="生成 SSH 密钥对" id="sourceContent1">
      <![CDATA[<b>使用主节点上的 <b>ssh-keygen</b> 生成一个 RSA 密钥对 (注意避免输入口令)。</b><br />
      <img style="max-width:100%;" src="//c1.staticflickr.com/3/2923/14086005918_0e28bff367_b.jpg" />
      <br />
      <img style="max-width:100%;" src="//c2.staticflickr.com/6/5525/14272256944_1659236551_b.jpg" />
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="margin-top:2em;">
      <![CDATA[
      <link href="//vjs.zencdn.net/4.6/video-js.css" rel="stylesheet" />
      <script src="//vjs.zencdn.net/4.6/video.js"></script>
      
      <video id="example_video_1" class="video-js vjs-default-skin"
        controls preload="auto" width="640" height="400"
        poster="https://dl.dropboxusercontent.com/s/olrd9un3hlzupjs/SSH-with-Mac-Terminal.jpg"
        data-setup='{"example_option":true}'>
        <source src="https://dl.dropboxusercontent.com/s/tptqduf5iibmktu/SSH-with-Mac-Terminal.mp4" type='video/mp4' />
       
       
        <track kind="captions" src="https://dl.dropboxusercontent.com/s/l6jxdcw32n4y29a/SSH-with-Mac-Terminal-cn.vtt" srclang="zh" label="Chinese" >
        <track kind="captions" src="https://dl.dropboxusercontent.com/s/2v8cy4ux66v58ti/SSH-with-Mac-Terminal-en.vtt" srclang="en" label="English" default>
        <p class="vjs-no-js">To view this video please enable JavaScript, and consider upgrading to a web browser that 
                            <a href="http://videojs.com/html5-video-support/" target="_blank">supports HTML5 video</a></p>
      </video>        
      ]]>
    </c:sourceContent>


    <c:sourceContent type="html" title="Hadoop 配置目录" id="sourceContent1" style="margin-top:2em;">
      <![CDATA[
      <img style="max-width:100%;" src="//c2.staticflickr.com/6/5579/14281021394_238533831e_b.jpg" /><br />
      <br />
      指定包括主节点在内的所有节点上的 jdk 安装路径 (即在 <b>hadoop-env.sh</b> 中定义 JAVA_HOME 环境变量使之指向 Java 安装目录)
      <br />
      <b>export JAVA_HOME=/user/share/jdk</b><br />
      <br />
      <b>1、</b>0.20 版本以前： Hadoop 的设置主要包含在 <b>hadoop-default.xml</b> 和 <b>hadoop-site.xml</b> 文件中。 <br />
      <b>2、</b>hadoop-default.xml 中包含了 Hadoop 会使用的默认设置，除非这些设置在 hadoop-site.xml 中被显示地重新定义。
      因此实际操作中只需要处理 hadoop-site.xml。<br />
      <b>3、</b>在 0.20 版本中：hadoop-site.xml 文件被拆分成 3 个 xml 文件。
      分别是：<b>core-site.xml、hdfs-site.xml、mapred-site.xml</b>。
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="单机模式" id="sourceContent1">
      <![CDATA[
     这种模式下所有 3 个 xml 文件均为空。当配置文件为空时，Hadoop 将完全运行在本地。
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="伪分布模式" id="sourceContent1" style="border-bottom:none;">
      <![CDATA[
     该模式增加了代码调试功能，允许程序员检查内存使用情况、HDFS 输入输出、以及其它的守护进程交互。<br />
     <br />
     <b>伪分布模式下 3 个 xml 文件示例：</b><br /><br /><br /><br />
     <i><b>core-site.xml</b></i>
      ]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>fs.default.name</name>
          <value>hdfs://localhost:9000</value>
          <description>
              The name of the default file system. 
              A URI whose scheme and authority determine the 
              FileSystem implementation.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<br /><i><b>mapred-site.xml</b></i>]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>mapred.job.tracker</name>
          <value>localhost:9001</value>
          <description>
              The host and port that the MapReduce job tracker runs at.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<br /><i><b>hdfs-site.xml</b></i>]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>dfs.replication</name>
          <value>1</value>
          <description>
              The actual number of replications can be specified when the file is created.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;">
      <![CDATA[<b style="color:red;">以上 3 个文件的配置中分别在 core-site.xml 和 mapred-site.xml 中指定了 NameNode 和 JobTracker 的主机名与端口。
      在 hdfs-site.xml 中指定了 HDFS 的默认副本数。</b>
      <br />
      <br />
      接下来需要在 masters 文件中指定 SNN 的位置，并在 slaves 中指定从节点的位置： <br />
      <b>cat masters<br />
      localhost
      <br />
      cat slaves <br />
      localhost</b>
      <br />
      <br />
      接下来检查机器是否允许对自己运行 ssh：<br />
      <b>ssh localhost</b>
      <br />
      如果不允许，需要用两行命令来安装：<br />
      <b>ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa <br />
      cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys </b>
       <br />
       <br />
      接下来输入一个命令格式化 HDFS：<br />
      <b>bin/hadoop namenode-format</b>
      <br />
      <br />
      接下来使用 start-all.sh 脚本装在守护进程，然后用 Java 的 jps 命令列出所有守护进程来验证安装是否成功：<br />
      <b>bin/start-all.sh<br />
      jps</b>
      <br />
      <br />
      <b style="color:gray;">26893 Jps<br />
      26832 TaskTracker<br />
      26620 SecondaryNameNode<br />
      26333 NameNode<br />
      26484 DataNode<br />
      26703 JobTracker</b><br />
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="全分布模式" id="sourceContent1" style="border-bottom:none;">
      <![CDATA[
      1、master -- 集群的主节点，驻留 NameNode 和 JobTracker 守护进程 <br />
      2、backup -- 驻留 SNN 守护进程 <br />
      3、hadoop1、hadoop2、hadoop3、..... -- 集群的从节点，驻留 DataNode 和 TaskTracker 守护进程 <br />
      <br /><br />
      在伪分布模式配置文件的基础上做进一步修改：
      ]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[
     <i><b>core-site.xml</b></i>
      ]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>fs.default.name</name>
          <value>hdfs://master:9000</value>   <!-- NameNode 所在节点 -->
          <description>
              The name of the default file system. 
              A URI whose scheme and authority determine the 
              FileSystem implementation.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<br /><i><b>mapred-site.xml</b></i>]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>mapred.job.tracker</name>
          <value>master:9001</value>      <!-- JobTracker 所在节点 -->
          <description>
              The host and port that the MapReduce job tracker runs at.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<br /><i><b>hdfs-site.xml</b></i>]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>dfs.replication</name>
          <value>3</value>        <!-- 增大 HDFS 备份数量 -->
          <description>
              The actual number of replications can be specified when the file is created.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;">
      <![CDATA[接下来更新 masters 和 slaves 文件用以指定其它守护进程的位置：<br />
      <b>cat master <br />
      backup<br />
      cat salves <br />
      hadoop1<br />
      hadoop2<br />
      hadoop3</b><br />
      <br />
      接下来将这些文件复制到集群上的所有节点，并格式化 HDFS 以准备存储数据：<br />
      <b>bin/hadoop namenode-format</b>
      <br /><br />
      接下来启动 Hadoop 守护进程：<br />
      <b>bin/start-all.sh </b><br />
      <br />
      接下来验证节点正在运行被指派的任务：<br />
      <b>[hadoop-user@master]$ jps <br />
      30879 JobTracker<br />
      30717 NameNode <br />
      30965 Jps <br />
      <br />
      [hadoop-user@backup]$ jps <br />
      2099 Jps
      1679 SecondaryNameNode <br />
      <br />
      [hadoop-user@hadoop1] jps <br />
      7101 TaskTracker <br />
      7617 Jps <br />
      6988 DataNode </b><br />
      <br />
      
      <b>至此，一个可用的集群就建成了!</b>]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>基于 Web 界面的集群用户界面</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="HDFS" id="sourceContent1">
      <![CDATA[。
      ]]>
    </c:sourceContent>




    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>Hadoop 组件 &#8212; HDFS</c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[HDFS 是一种分布式文件系统，专为 MapReduce 这类框架下的大规模分布式数据处理而设计的。可以把一个 100TB 的大数据集在 Hadoop 中存储为单个文件。<br />
      <b style="color:red;">Hadoop 提供了一套与 Linux 文件命令类似的命令行工具，用于与 HDFS 交互。</b>]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="html" title="HDFS 文件操作" id="sourceContent1">
      <![CDATA[<b>假设已经完成了 HDFS 格式化，并启动了一个 HDFS 文件系统 (可以使用伪分布模式)。</b>
      <br />  <br />
      
      <b style="color:green;">基本文件命令：</b>
      <br />
      <b>hadoop fs -cmd &lt;args&gt;</b><br /><br />
      <b>hadoop fs -ls &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 文件列表</b>
      <br />  
      <b>将会产生如下输出：</b><br />
      Fount N items <br />
      -rw-r--r-- <b>1</b> keesh supergroup 264 2014-05-26 18:17 /user/keesh/filename.txt<br />
      其中 1 代表复制因子 (伪分布式下永远为 1，集群环境通常为 3。)
      <br />  <br />
      
      <b style="color:green;">添加文件和目录：</b>
      <br />
      <b>HDFS 有一个默认的工作目录 /user/$USER，($USER 是登陆名，并且这个目录不会自动建立，可以 mkdir 创建它。)</b><br /><br />
      <b>hadoop fs -mkdir /user/keesh &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 创建目录 (将自动创建父级目录)</b><br />
      <b>hadoop fs -lsr /  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 查看所有文件和子目录</b><br />
      <b>hadoop fs -put example.txt .  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 将 example.txt 复制到 Hadoop 的默认工作目录 (. 代表默认目录)</b><br />
      <b>hadoop fs -put example.txt /user/keesh  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 等价于上面的命令</b><br />
      <b>hadoop fs -get example.txt .  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 将 example.txt 文件从 Hadoop 中复制到本地当前目录</b><br />
      <b>hadoop fs -cat example.txt   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 显示 example.txt 文件的内容</b><br />
      <b>hadoop fs -rm example.txt   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 删除文件或目录</b><br />
      ]]>
    </c:sourceContent>


    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>HDFS Java API 实践 (编程读写 HDFS)</c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[Hadoop 的基本文件操作包括： open、read、write、close、delete、exists、mkdirs、rename 等。]]>
      </c:desc1>
      <c:desc1>
        <![CDATA[Hadoop 的文件 API 同时也可以操作本地系统文件。]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="" title="PutMerge 程序" id="sourceContent1">
      <![CDATA[/*
    1、Hadoop 中有一个 getmerge 命令，用于把一组 HDFS 文件在复制到本地计算机的过程中进行合并。
    2、PutMerge 程序用于把一组文件放到 HDFS 的过程中合并文件。
    3、Hadoop 处理单个大文件比处理许多小文件更有效率。
*/
import java.io.IOException;
      
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FSDataOutputStream;
import org.apache.hadoop.fs.FileStatus;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
      
public class PutMerge {
    public static void main (String[] args) throws IOException {
        Configuration conf = new Configuration();
          
        FileSystem hdfs = FileSystem.get(conf);
    
        // 创建一个专用于本地系统文件的 FileSystem 对象
        FileSystem local = FileSystem.getLocal(conf);
          
        Path inputDir = new Path(args[0]);
        Path hdfsFile = new Path(args[1]);
          
        try {
            // 取得本地文件列表 
            // (FileStatus 对象用来存储文件和目录的元数据： 如文件长度、权限、修改时间等)
            FileStatus[] inputFiles = local.listStatus(inputDir);
              
            // 生成 HDFS 输出流
            FSDataOutputStream out = hdfs.create (hdfsFile);
              
            for (int i = 0; i < inputFiles.length; i++) {
                System.out.println (inputFiles[i].getPath().getName());
                  
                // 取得本地文件输入流
                FSDataInputStream in = local.open(inputFiles[i].getPath());
                  
                byte buffer[] = new byte[256];
                int bytesRead = 0;
                  
                while ((bytesRead = in.read(buffer)) > 0) {
                    out.write (buffer, 0, bytesRead);
                }
                  
                in.close();
            }
              
            out.close();
        } catch (IOException) {
            e.printStackTrace();
        }
    }
}]]>
    </c:sourceContent>

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>Hadoop 组件 &#8212; MapReduce</c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[<span style="font-weight:bold;font-size:1.3em;font-style:rgb(241, 91, 20);">通常的 MapReduce 程序都不太大，可以采用特定的结构来简化开发，而把大部分工作放在对数据流的思考上<span>]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent expand="false" type="html" title="剖析 MapReduce 程序" id="sourceContent1">
      <![CDATA[
      MapReduce 程序通过操作键/值对来处理数据，一般形式为：<br /><br />
      <b style="color:green;font-size:1.3em;">map: (K1,V1) &rarr; list(K2,V2) <br />
      reduce: (K2, list(V2)) &rarr; list(K3, V3)</b>
      <br /><br />
      <img style="width:70%;" src="//c2.staticflickr.com/6/5498/14309905763_a215ed134a_b.jpg" />
      ]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="Hadoop 数据类型" id="sourceContent1">
      <![CDATA[
          <b>因为标准的 Java 数据类型不支持在集群上移动，所以 MapReduce 框架提供了一种序列化键/值对的方法。
          只有那些支持这种序列化的类能够在这个框架中充当键或者值。</b>
          
          <br />
          <br />
          实现了 Writable 接口的类可以是值，而实现了 WritableComparable<T> 接口的类既可以是值也可以是键。
          由于在 Reduce 阶段会进行排序，所以对于键而言需要实现 Comparable<T> 接口。
          
          <br />
          <br />
          Hadoop 实现了所有基本数据类型，这些数据类型支持序列化的方法，所以可以在集群上移动。

          <br />
          <br />
          <b>键/值对经常使用的数据类型列表:</b>
          <table>
            <thead>
              <tr>
                <th style="padding:.3em .8em;padding-left:0;font-weight:bold;">类</th>
                <th style="padding:.3em .8em;padding-left:0;font-weight:bold;">描述</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="padding:.3em .8em;padding-left:0;">BooleanWritable</td>
                <td style="padding:.3em .8em;padding-left:0;">标准布尔变量的封装</td>
              </tr>
              <tr>
                <td style="padding:.3em .8em;padding-left:0;">ByteWritable</td>
                <td style="padding:.3em .8em;padding-left:0;">单字节数的封装</td>
              </tr>
              <tr>
                <td style="padding:.3em .8em;padding-left:0;">DouableWritable</td>
                <td style="padding:.3em .8em;padding-left:0;">双字节数的封装</td>
              </tr>
              <tr>
                <td style="padding:.3em .8em;padding-left:0;">FloatWritable</td>
                <td style="padding:.3em .8em;padding-left:0;">浮点数的封装</td>
              </tr>
              <tr>
                <td style="padding:.3em .8em;padding-left:0;">IntWritable</td>
                <td style="padding:.3em .8em;padding-left:0;">整数的封装</td>
              </tr>
              <tr>
                <td style="padding:.3em .8em;padding-left:0;">Text</td>
                <td style="padding:.3em .8em;padding-left:0;">使用 UTF8 格式的文本封装</td>
              </tr>
              <tr>
                <td style="padding:.3em .8em;padding-left:0;">NullWritable</td>
                <td style="padding:.3em .8em;padding-left:0;">无键/值时的占位符</td>
              </tr>
            </tbody>
          </table>
          <br />
          <b>For more information, please reference <a style="color:rgb(14, 185, 252);" href="/blog/articles/big-data/creating-custom-hadoop-writable-data-type.shtml">Creating Custom Hadoop Writable Data Type</a></b>
          ]]>
    </c:sourceContent>
    <c:sourceContent expand="false" type="" title="自定义 Hadoop 数据类型: Edge" id="sourceContent1">
      <![CDATA[/*
  该 Edge 类型代表两个城市之间的航线。
  该 Edge 类实现了 Writable 接口中的 readFields() 和 write() 方法,
  它们与 Java 中的 DataInput 和 DataOutput 类一起用于类中内容的串行化。
  利用该 Edge 自定义数据类型我们可以开始数据处理流程的第一阶段 (即 mapper 阶段)。
*/
public class Edge implements WritableComparable<Edge> {
    private String departureNode;
    private String arrivalNode;
  
    public String getDepartureNode() { return departureNode; }
    
    
    // 说明如何读入数据
    @Override
    public void readFields (DataInput in) throws IOException {
        departureNode = in.readUTF();
        arrivalNode = in.readUTF();
    }
    
    // 说明如何写出数据
    @Override 
    public void write (DataOutput out) throws IOException {
        out.writeUTF(departureNode);
        out.writeUTF(arrivalNode);
    }
    
    @Override
    public int compareTo (Edge o) {
        int i = departureNode.compareTo(o.departureNode);
      
        if (i != 0) {
            return i;  
        }
        
        return arrivalNode.compareTo(o.arrivalNode);
    }
}]]>
    </c:sourceContent>
    <c:sourceContent type="" title="Partitioner: 重定向 Mapper 输出" id="sourceContent1" style="border-bottom:none;">
      <![CDATA[/*
    为了进行并行计算, Hadoop 将生成多个 reducer 来处理 mapper 的输出。
    当使用多个 reducer 时, Hadoop 将对键进行散列来确定 mapper 应该把键/值对输出给谁。
    Hadoop 通过 HashPartitioner 类来强制执行散列，但有时默认使用的 HashPartitioner 会让你出错，为了避免出错需要自定义 Partitioner。
      
    例如: 我们希望具有相同离港地的所有 edge 被送往相同的 reducer。
*/
      
public class EdgePartitioner implements Partitioner<Edge, Writable> {
    @Override
    public int getPartition (Edge key, Writable value, int numPartitions) {
        return key.getDepartureNode().hashCode() % numPartitions;
    }
          
    @Override
    public void configure (JobConf conf) {  }
}

/*
    一个自定义的 Partitioner 只需要实现 configure() 和 getPartition() 两个函数。
    前者将 Hadoop 对作业的配置应用在 partitioner 上，而后者返回一个介于 0 和 reduce 任务数之间的整数，该整数指向键/值对将要发送到的 reducer。
*/]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;">
      <![CDATA[
      <b>下图展示了 MapReduce 数据流，重点说明了分区 (Partitioning) 和洗牌 (shuffling): </b><br />
      <img style="max-width:100%;" src="https://c2.staticflickr.com/4/3860/14613855679_4f21435542.jpg" />
      <br />
      ]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="Combiner: 本地 reduce" id="sourceContent1">
      <![CDATA[Combiner 即在分发 mapper 结果之前做一下 "本地 reduce"。<br />
      <b>例如: 洗牌一次 (the, 23) 键/值对比洗牌 23 次 (the, 1) 键/值对更为高效。</b>]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>读和写</c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[
        <span style="margin-bottom:.5em;display:block;">输入数据通常驻留在较大的文件中，通常几十或数百 GB，甚至更大。MapReduce 处理的基本原则之一是将输入数据分割成块。这些块可以在多台机器上并行处理。</span>
        <span style="margin-bottom:.5em;display:block;">在 Hadoop 术语中，这些块被称为输入分片 (Input Split)。每个分片应该足够小以实现更细粒度的并行。</span>
        <span style="margin-bottom:.5em;display:block;">每个分片也不能太小，否则启动与停止各个分片处理所需的开销将占去很大一部分时间。</span>
        <span style="margin-bottom:.5em;display:block;">Hadoop 的文件系统实现了 FSDataInputStream 类用于读取文件，该类扩展了 java.io.DataInputStream 并支持随机读取。</span>
        <span style="margin-bottom:.5em;display:block;">HDFS 按块存储文件并分布在多台机器上，每个文件块为一个分片。由于不同机器会存储不同的块，如果每个分片/块都由它所驻留的机器进行处理，就自动实现了并行。</span>
        <span style="margin-bottom:.5em;display:block;">MapReduce 操作是基于键/值对的，并且 Hadoop 默认地将输入文件中的每一行视为一个记录，键/值对分别对应该行的字节偏移 (Key) 和内容 (Value)。并不是所有记录都如此记录，所以 Hadoop 允许自定义格式。</span>
        <br /><span style="margin-bottom:.5em;display:block;font-weight:bold;color:blue;">注意: 输入分片是一种记录的逻辑划分，而 HDFS 数据块是对输入数据的物理划分。
        当二者一致时效率会非常高，但在实际应用中从未达到完全一致。记录可能会跨过数据块的边界。<br />
        Hadoop 确保全部记录都被处理，处理特定分片的计算节点会从一个数据块中获取记录的一个片段，该数据块可能不是该记录的 "主" 数据块，而会存放在远端。</span>

        ]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="html" title="InputFormat" id="sourceContent1">
      <![CDATA[
      <table>
        <thead>
            <tr>
                <th>InputFormat</th>
                <th>描述</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>TextInputFormat</td>
                <td>
                    在文本文件中的每一行都为一个记录。键 (key) 为一行的字节偏移，而值 (Value) 为一行的内容。<br />
                    Key: LongWritable<br />
                    Value: Text
                </td>
            </tr>
            <tr>
                <td>KeyValueTextInputFormat</td>
                <td>
                    在文本文件中的每一行都为一个记录。
                    以每行的第一个分隔符为界，分隔符之前的是键，之后的是值。
                    分离器在属性 <br />
                    <b>key.value.separator.in.input.line </b>
                    <br />
                    中设定，默认为 \t。
                    <br />
                    Key: Text <br />
                    Value: Text
                </td>
            </tr>
            <tr>
                <td>SequenceFileInputFormat</td>
                <td>
                    用于读取序列文件的 InputFormat，键和值由用户定义。
                    序列文件为 Hadoop 专用的压缩二进制文件格式。
                    它专用于一个 MapReduce 作业和其他 MapReduce 作业之间传送数据。
                    <br />
                    Key: K (Customer) <br />
                    Value:  V (Customer)
                </td>
            </tr>
            <tr>
                <td>NLineInputFormat</td>
                <td>
                    与 TextInputFormat 相同，但每个分片一定有 N 行。N 在属性 <br />
                    <b>mapred.line.input.format.linespermap </b>
                    <br />中设定，默认为 1。
                    <br />
                    Key: LongWritable<br />
                    Value: Text
                </td>
            </tr>
        </tbody>
      </table>]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="KeyValueTextInputFormat" id="sourceContent1" 
                     style="margin-top:2em;background-color:white;" 
                     titleStyle="background-color:white;color:black;padding:1em 0;font-size:1.2em;color:rgb(201, 136, 0);font-weight:normal;"
                     bodyStyle="padding:0;">
      <![CDATA[KeyValueTextInputFormat 在想对结构化的输入文件中使用，由一个预定义的字符将每行的键与值分开。
      <br />
      <br />
      例如由一个制表符分隔的，有时间戳和 URL 组成的数据文件:
      <br />
      17:16:18  http://hadoop.apache.org/ <br />
      17:16:19  http://hadoop.apache.org/ <br />
      17:16:20  http://wiki.apache.org/ <br />
      17:16:21  http://www.maxim.com/ <br />
      .... <br />
      <br />
      设置 JobConf 对象使用 KeyValueTextInputFormat 类读取该文件:
      <br />
      <b>conf.setInputFormat(KeyValueTextInputFormat.class);</b>
      <br />
      <br />
      在此种情况下 mapper 将文档的每一行视作一条记录.
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="SequenceFileInputFormat" id="sourceContent1"
                     style="margin-top:2em;background-color:white;"
                     titleStyle="background-color:white;color:black;padding:1em 0;font-size:1.2em;color:rgb(201, 136, 0);font-weight:normal;"
                     bodyStyle="padding:0;">
      <![CDATA[Hadoop 提供了高效的二进制压缩文件格式，称为序列文件。
      <br />
      这个格式为 Hadoop 处理做了优化，当链接多个 MapReduce 作业时，SequenceFileInputFormat 是首选格式。
      <br />
      <br />
      序列文件的键和值均可由用户自定义。输出和输入类型必须匹配，Mapper 实现和 map () 方法均需采用正确的输入类型。
      ]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>自定义 InputFormat &#8212; InputSplit 和 RecordReader </c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[
        ]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="" title="InputFormat.java" id="sourceContent1">
      <![CDATA[public interface InputFormat<K, V> {
    InputSplit[] getSplits (JobConf conf, int numSplits) throws IOException;
    RecordReader<K, V> getRecordReader (InputSplit split,
                                        JobConf conf,
                                        Reporter reporter) throws IOException;
}]]>

     
    </c:sourceContent>


    <c:sourceContent type="html" title="InputFormat 所需执行的两个功能" id="sourceContent1"
                        style="margin-top:1em;background-color:white;"
                        titleStyle="background-color:white;color:black;padding:1em 0;font-size:1.2em;color:rgb(201, 136, 0);font-weight:normal;"
                        bodyStyle="padding:0;">
      <![CDATA[
      <ul style="list-style:decimal;padding-left:1.5em;">
          <li style="margin-bottom:.5em;">
              <span style="display:block;margin-bottom:.5em;">确定所有用于输入数据的文件，并将之分割为输入分片。每个 map 任务分配一个分片。</span>
              <span style="display:block;margin-bottom:.5em;">我们当然不想去考虑如何将文件划分为分片。<b style="color:red;">所以在创建自己的 InputFormat 类时，通常从 FileInputFormat 类中继承一个子类。</b></span>
              <span style="display:block;margin-bottom:.5em;">FileInputFormat 实现了 getSplits() 方法，但是保留了 getRecordReader() 抽象让子类填写。</span>
              <span style="display:block;margin-bottom:.5em;">
                  FileInputFormat 中实现的 getSplits() 方法把输入数据粗略地划分为一组分片，分片数目在 numSplits 中设定，且每个分片的大小必须大于 mapred.min.split.size 个字节，但小于文件系统的块。
                  
              </span>
              <span style="display:block;margin-bottom:.5em;">
                  FileInputFormat 有一定的 protected 方法，子类可以通过覆盖改变其行为，其中一个就是 boolean isSplitable (FileSystem fs, Path filename) 方法。
                  
              </span>
              <span style="display:block;margin-bottom:.5em;">
                  该方法决定是否可以将给定文件分片，默认实现总是返回 true，因此大于一个分块 (大于 64M )的文件都要分片。
                  
              </span>
              <span style="display:block;margin-bottom:.5em;">
                  有时我们希望一个文件就是一个分块，这时你就可以覆盖 isSplitable() 来返回 false。
                  
              </span>
          </li>
          <li>              
              <span style="display:block;margin-bottom:.5em;">
                把一个输入分片解析为记录，再把每个记录解析为一个键/值对。
              </span>

          </li>
      </ul>
      ]]>
    </c:sourceContent>

    <c:sourceContent type="" title="RecordReader&lt;K, V&gt;" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[public interface RecordReader<K, V> {
    boolean next (K key, V value) throws IOException;
    K createKey ();
    V createValue ();
    
    long getPos () throws IOException;
    public void close () throws IOException;
    float getProgress () throws IOException;
}]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="预定义 RecordReader &#8212; LineRecordReader 和 KeyValueLineRecordReader" id="sourceContent1"
                        style="margin-top:1em;background-color:white;"
                        titleStyle="background-color:white;color:black;padding:1em 0;font-size:1.2em;color:rgb(201, 136, 0);font-weight:normal;"
                        bodyStyle="padding:0;">
      <![CDATA[
      <span style="display:block;margin-bottom:.5em;">LineRecordReader 实现 RecordReader<LongWritable, Text>, 它在 TextInputFormat 中被用于每次读取一行，以字节偏移为键，以行内容为值。</span>
      <span style="display:block;margin-bottom:.5em;">KeyValueLineRecordReader 被用在 KeyValueTextInputFormat 中。</span>      
      <span style="display:block;margin-bottom:.5em;"><b style="color:red;">大多数情况下，自定义 RecordReader 是基于现有实现的封装，并把大多数操作放在 next () 中。</b></span>      
      ]]>
    </c:sourceContent>
    
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>自定义 InputFormat &#8212; TimeUrlTextInputFormat </c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[用特定的类型读取记录，而不是普通的 Text 类型。<br />
        它从数据文件中读取以制表符分隔的时间戳和 URL，但是把 URL 作为 URLWritable 类型，把时间戳作为 CalendarWritableComparable 类型。
        
        ]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="" title="TimeUrlTextInputFormat.java" id="sourceContent1">
      <![CDATA[public class TimeUrlTextInputFormat extends FileInputFormat<Text, URLWritable> {
    public RecordReader<Text, URLWritable> getRecordReader (
                                  InputSplit input, JobConf job, Reporter reporter)
                                  throws IOException {
        return new TimeUrlLineRecordReader (job, (FileSplit) input);                          
    }
}]]>


    </c:sourceContent>

    <c:sourceContent type="" title="URLWritable.java" id="sourceContent1">
      <![CDATA[public class URLWritable implements Writable {
    protected URL url;
    
    public URLWritable () {
        
    }
    
    public URLWritable (URL url) {
        this.url = url;
    }
    
    public void write (DataOutput out) throws IOException {
        out.writeUTF (url.toString());
    }
    
    public void readFields (DataInput in) throws IOException {
        url = new URL (in.readUTF());
    }
    
    public void set (String s) throws MalformedURLException {
        url = new URL (s);
    }
}]]>


    </c:sourceContent>

    <c:sourceContent type="" title="TimeUrlLineRecordReader.java" id="sourceContent1">
      <![CDATA[public class TimeUrlLineRecordReader implements RecordReader<Text, URLWritable> {
    
    private KeyValueLineRecordReader lineReader;
    private Text lineKey, lineValue;
    
    public TimeUrlLineRecordReader (JobConf job, FileSplit split) 
                      throws IOException {
        lineReader = new KeyValueLineRecordReader (job, split);
        
        lineKey = lineReader.createKey();
        lineValue = lineReader.createValue();
    }
    
    public boolean next (Text key, URLWritable value) 
                      throws IOException {
        if (!lineReader.next(lineKey, lineValue)) {
            return false;
        }    
    
        key.set(lineKey);
        value.set(lineValue.toString());
    
        return true;
    }
    
    public Text createKey () {
        return new Text ("");
    }
    
    public URLWritable createValue () {
        return new URLWritable();
    }
    
    public long getPos () throws IOException {
        return lineReader.getPos();
    }
    
    public float getProgress () throws IOException {
        return lineReader.getProgress();
    }
    
    public void close () throws IOException {
        lineReader.close();
    }
}]]>


    </c:sourceContent>
    

    

    

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>
  
  <c:entry style="margin-top:2em;">
    <c:title>OutputFormat</c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[
              <span style="display:block;margin-bottom:.5em;">当 MapReduce 输出数据到文件时，使用的是 OutputFormat 类，每个 reducer 将它的输出写入自己的文件中，所以输出无需分片。</span>
              <span style="display:block;margin-bottom:.5em;">
                  输出文件放在一个公用目录中，通常命名为 part-nnnnn, 这里的 n 是 reducer 的分区 ID。
              </span>
              <span style="display:block;margin-bottom:.5em;">
                  大部分的 OutputFormat 类都是从 FileOutputFormat 抽象类继承来的, 可以通过调用 JobConf 对象中的 setOutputFormat() 定制 OutputFormat。
              </span>
        ]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="html" title="主要的 OutputFormat 类，默认为 TextOutputFormat " id="sourceContent1"
                     titleStyle="background-color:white;padding-left:0;color:black;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
      <table>
          <thead>
              <tr>
                  <th>OutputFormat</th>
                  <th>描述</th>
              </tr>
          </thead>
          <tbody>
              <tr>
                <td>TextOutputFormat&lt;K,V&gt; </td>
                <td>
                    <span style="display:block;margin-bottom:.5em;">
                      将每个记录写为一行文本，键和值以字符串的形式写入，并以制表符分隔。这个分隔符可以在属性 <b style="color:green;">mapred.textoutputformat.separator</b> 中修改
                    </span>
                    <span style="display:block;margin-bottom:.5em;">
                      每个记录的键和值通过 toString() 被转换为字符串，并以制表符 (\t) 分隔。  
                    </span>
                </td>
              </tr>
              <tr>
                <td>SequenceFileOutputFormat&lt;K,V&gt; </td>
                <td>
                    以 Hadoop 专有序列文件格式写入键/值对，与 SequenceFileInputFormat 配合使用。
                </td>
              </tr>
              <tr>
                <td>NullOutputFormat&lt;K,V&gt; </td>
                <td>
                    <span style="display:block;margin-bottom:.5em;">无输出，该类没有继承 FileOutputFormat 而是简单地实现了 OutputFormat 类。</span>
                    <span style="display:block;margin-bottom:.5em;">
                      OutputFormat 处理的是数据库而并非文件，在类的层次关系中 OutputFormat (InputFormat) 类是区别于 FileOutputFormat (FileInputFormat) 的一个独立分支 
                    </span>
                    <span style="display:block;margin-bottom:.5em;">在 Hadoop 中，处理数据ku的类分别是: DBInputFormat 和 DBOutputFormat。</span>
                </td>
              </tr>
          </tbody>
      </table>  
        ]]>
    </c:sourceContent>
    <c:comment style="color:black;">
      <c:comment1>
        <![CDATA[
          <span style="display:block;margin-bottom:.5em;">
              TextOutputFormat 的输出类型可以被 KeyValueTextInputFormat 识别。如果把键的类型设置为 NullWritable 类型，则其输出类型可以被 TextInputFormat 识别。
              TextOutputFormat 采用可被 KeyValueTextInputFormat 识别的格式输出数据。
          </span>
          <span style="display:block;margin-bottom:.5em;">
              如果想完全禁止输出，应该使用 NullOutputFormat。<br />
              如果让 reducer 采用自己的方式输出，并且不需要 Hadoop 写任何的附加文件，则可以限制 Hadoop 的输出。
          </span>
          <span style="display:block;margin-bottom:.5em;">
              SequenceFileOutputFormat 以序列文件格式输出数据，使其可以通过 SequenceFileInputFormat 读取。<br />
              它有助于通过中间数据将 MapReduce 作业串接起来。
          </span>]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>编写 MapReduce 基础程序</c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="html" title="取得数据集" id="sourceContent1">
      <![CDATA[
      从美国国家经济研究局取得专利数据集： <a target="_blank" href="http://www.nber.org/patents/">http://www.nber.org/patents/</a> <br />
      专利引用数据集: <a href="http://www.nber.org/patents/acite75_99.zip">http://www.nber.org/patents/acite75_99.zip</a><br />
      专利描述数据集: <a href="http://www.nber.org/patents/apat63_99.zip">http://www.nber.org/patents/apat63_99.zip</a> <br />
      <br />
      
      <b>专利引用数据集关系图</b><br />
      <img src="https://c2.staticflickr.com/4/3894/14157325750_3bac35529f_b.jpg" style="max-width:100%;" />
      
      
      <br />
      <b>专利描述数据集中前 10 个属性的定义</b>
      <br />
      <table>
          <thead>
            <tr>
              <th style="padding:.2em .5em;padding-left:0;font-weight:bold;">属性名</th>
              <th style="padding:.2em .5em;padding-left:0;font-weight:bold;">内容</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">PATENT</td>
              <td style="padding:.2em .5em;padding-left:0;">专利号</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">GTEAR</td>
              <td style="padding:.2em .5em;padding-left:0;">批准年</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">GDATE</td>
              <td style="padding:.2em .5em;padding-left:0;">批准日 (自 1960 年 1 月 1 日以来的日期)</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">APPYEAR</td>
              <td style="padding:.2em .5em;padding-left:0;">申请年 (仅适用于自 1967 年起批准的专利)</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">COUNTRY</td>
              <td style="padding:.2em .5em;padding-left:0;">第一发明人国家</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">POSTATE</td>
              <td style="padding:.2em .5em;padding-left:0;">第一发明人所在州 (若国家为美国)</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">ASSIGNEE</td>
              <td style="padding:.2em .5em;padding-left:0;">专利权人 (即专利拥有者) 数字标识</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">ASSCODE</td>
              <td style="padding:.2em .5em;padding-left:0;">专利权人类型，1位 (1-9)。(专利权人类型包括: 美国个人、美国政府、美国组织、非美国个人)</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">CLAIMS</td>
              <td style="padding:.2em .5em;padding-left:0;">声明数目 (仅适用于自 1975 年授权的专利)</td>
            </tr>
            <tr>
              <td style="padding:.2em .5em;padding-left:0;">NCLASS</td>
              <td style="padding:.2em .5em;padding-left:0;">3位，表示主要专利类型</td>
            </tr>
          </tbody>
      </table>
      ]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="构建 MapReduce 程序基础模板" id="sourceContent1">
      <![CDATA[
      <b>通常在一个现有的 MapReduce 程序基础上撰写新的 MapReduce 程序。</b> <br /><br />
      编写一个 MapReduce 程序读取专利引用数据集并对它进行排序。对于每一个专利，我们希望找到所有引用它的专利并进行合并。
      输出大致如下:<br /><br />
      <b>
        10000 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 4539112 <br />
        100000 &nbsp;&nbsp;&nbsp; 5031388 <br />
        1000067 &nbsp; 5312208, 4944640, 5071294 <br />
        .......
      </b>
      <br /><br />
      通过观察我们可以发现专利 5312208、 4944640 和 5071294 引用了专利 1000067
        ]]>
    </c:sourceContent>
    <c:sourceContent expand="false" type="" title="典型 Hadoop 程序模板 (旧版 API)" id="sourceContent1" style="border-bottom:none;">
      <![CDATA[public class MyJob extends Configured implements Tool {
    public static class MapClass extends MapReduceBase
                                 implements Mapper<Text, Text, Text, Text> {
        public void map(Text key, Text value
                        , OutputCollector<Text, Text> output
                        , Reporter reporter) throws IOException {
            output.collect(value, key);                 
        }
    }
    
    public static class Reduce extends MapReduceBase 
                               implements Reducer<Text, Text, Text, Text> {
        public void reduce(Text key, Iterator<Text> values
                           , OutputCollector<Text, Text> output
                           , Reporter reporter) throws IOException {
            String csv = "";
            while (values.hasNext()) {
                if (csv.length() > 0) csv += ",";
                csv += values.next().toString();
            }
            
            output.collect(key, new Text(csv));
        }
    }
    
    public int run (String[] args) throws Exception {
        Configuration conf = getConf();
        JobConf job = new JobConf (conf, MyJob.class);
        
        Path in = new Path(args[0]);
        Path out = new Path(args[1]);
        
        FileInputFormat.setInputPaths(job, in);
        FileOutputFormat.setOutputPath(job, out);
        
        job.setJobName("MyJob");
        job.setMapperClass(MapClass.class);
        job.setReducerClass(Reduce.class);
        
        job.setInputFormat(KeyValueTextInputFormat.class);
        job.setOutputFormat(TextOutputFormat.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        job.set("key.value.separator.in.input.line", ",");
        
        JobClient.runJob(job);
        
        return 0;
    }
    
    public static void main(String[] args) throws Exception {
        int res = ToolRunner.run(new Configuration(), new MyJob(), args);
        
        System.exit(res);
    }
}]]>
    </c:sourceContent>
    <c:sourceContent expand="false" type=""
                     title="典型 Hadoop 程序模板 (新版 API 0.20)"
                     id="sourceContent1"
                     style="border-bottom:none;">
      <![CDATA[public class MyJob extends Configured implements Tool {
    public static class MapClass extends Mapper<LongWritable, Text, Text, Text> {
        public void map(LongWritable key, Text value
                        , Context context) 
                        throws IOException, InterruptedException {
            String[] citation = value.toString().split(",");
            context.write(new Text(citation[1]), new Text(citation[0]));
        }
    }
    
    public static class Reduce extends Reducer<Text, Text, Text, Text> {
        public void reduce(Text key, Iterable<Text> values
                           , Context context) 
                           throws IOException, InterruptedException {
            String csv = "";
            
            for (Text val:values) {
              if (csv.length() > 0) csv += ",";
              csv += val.toString();
            }
            
            context.write(key, new Text(csv));
        }
    }
    
    public int run (String[] args) throws Exception {
        Configuration conf = getConf();
        
        Job job = new Job(conf, "MyJob");
        job.setJarByClass(MyJob.class);
        
        Path in = new Path(args[0]);
        Path out = new Path(args[1]);
        
        FileInputFormat.setInputPaths(job, in);
        FileOutputFormat.setOutputPath(job, out);
        
        job.setMapperClass(MapClass.class);
        job.setReducerClass(Reduce.class);
        
        /*
          由于旧版 API 中的 KeyValueTextInputFormat 类没有被移植到新版 API 中，所以新版中不得不使用 TextInputFormat
        */
        //job.setInputFormat(KeyValueTextInputFormat.class);
        job.setInputFormatClass(TextInputFormat.class);
        job.setOutputFormatClass(TextOutputFormat.class);
        
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        
       // job.set("key.value.separator.in.input.line", ",");
        
        System.exit(job.waitForCompletion(true) ? 0 : 1);
        
        return 0;
    }
    
    public static void main(String[] args) throws Exception {
        int res = ToolRunner.run(new Configuration(), new MyJob(), args);
        
        System.exit(res);
    }
}]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;color:green;">
      <![CDATA[
     <b>Hadoop 要求 Mapper 和 Reducer 必须是它们自身的静态类。该模板将它们包含在 MyJob 类中作为内部类。</b> <br />
     <b>这些内部类是独立的，通常不与 MyJob 类进行交互 (放在一个文件中只是为了简化代码管理)。</b> <br />
     <b>在作业执行期间，采用不同 JVM 的各类节点复制并运行 Mapper 和 Reducer，其它的作业类仅在客户机上执行。</b> <br />
     <b>run() 方法是框架的核心，也称为 driver。</b> 
     <br />
     <br />
     <b>Configure 类为作业提供 configure() 和 close() 方法, 用于建立和清除 map (reduce) 任务。除非是更高级的作业，否则不需要覆盖他们。</b> 
     <br />
     <br />
     <b style="color:red;">1. 在 Mapper 和 Reducer 之间保持 K2 和 V2 类型的一致。 <br />
     2. 确保在 Mapper 和 Reducer 中使用的键值类型与在 driver 中设置的输入格式、输出键的类、输出值得类保持一致。<br />
     3. 使用 KeyValueTextInputFormat 意味着 K1 和 V1 必须均为 Text 类型。<br />
     4. driver 必须调用 setOutputKeyClass() 和 setOutputValueClass() 分别指定 K2 和 V2 的类。</b> 
        ]]>
    </c:sourceContent>
    <c:sourceContent type="" title="Mapper 类和 Reducer 类的定义如下" id="sourceContent1" style="border-top:none;color:green;">
      <![CDATA[public static class MapClass extends MapReduceBase implements Mapper<K1, V1, K2, V2> {
    public void map (K1 key, V1 value, OutputCollector<K2, V2> output, Reporter reporter) 
                                      throws IOException { }  
}
          
public static class Reduce extends MapReduceBase implements Reducer<K2, V2, K3, V3> {
    public void reduce (K2 key, Iterator<V2> values, OutputCollector<K3, V3> output, Reporter reporter) 
                                            throws IOException { }
}]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="driver" id="sourceContent1" style="border-top:none;color:green;">
      <![CDATA[
    <ul style="list-style:disc;margin-left:1.5em;">
      <li><b>在 driver 中实例化、配置并传递一个 JobConf 对象命名的作业给 JobClient.runJob() 用以启动 MapReduce 作业。(JobClient 与 JobTracker 通信让该作业在集群上启动。)</b></li>
      <li><b>Driver 需要在作业中为每个作业定制基本参数，包括输入路径、输出路径、Mapper 类和 Reducer 类。JobConf 对象将保持作业运行所需的全部配置参数。</b></li>
      <li><b>每个作业可以重置默认的作业属性，例如：InputFormat、OutputFormat 等。也可以调用 JobConf 对象中的 set() 方法填充任意的配置参数。</b></li>
      <li><b>一旦传递 JobConf 对象到 JobClient.runJob()，它就被视为作业的总体规划，成为决定这个作业如何运行的蓝本。</b></li>
      
      <li><b>除了在 Mapper 和 Reducer 之间保持 K2/V2 的类型一致，还需确保在 Mapper 和 Reducer 中使用的键值类型与在 driver 中设置的格式、输出的键/值类保持一致。</b></li>
      <li><b>使用 KeyValueTextInputFormat 意味着 K1/V1 必须均为 Text 类型。Driver 必须调用 setOutputKeyClass() 和 setOutputValueClass() 分别指定 K2 和 V2 的类。</b></li>
      <li><b>所有键与值得类型必须是 Writable 的子类型，以确保 Hadoop 的序列化接口可以把数据分布在集群上发送。</b></li>
    </ul>
        ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="编译并执行该 Job" id="sourceContent1" style="border-top:none;color:green;">
      <![CDATA[
    。。。
        ]]>
    </c:sourceContent>

    <c:sourceContent type="" title="修改程序模板以实现计数" id="sourceContent1" style="border-top:none;">
      <![CDATA[public static class Reduce extends MapReduceBase 
                                          implements Reducer<Text, Text, Text, IntWritable> {
        public void reduce (Text key, Iterator<Text> values
                            , OutputCollector<Text, IntWritable> output
                            , Reporter reporter) 
                                              throws IOException {
            int count = 0;
            
            while (values.hasNext()) {
                values.next();
                count++;
            }
            
            output.collect(key, new IntWritable(count));
        }
    }
        ]]>
    </c:sourceContent>
    
    <c:sourceContent type="html" title="生成引用计数的直方图" id="sourceContent1" 
                     style="border-top:none;margin-top:3em;background-color:white;margin-bottom:2em;"
                     titleStyle="color:rgb(156, 0, 230);background-color:white;padding-left:0;font-size:2.5em;margin-bottom:.5em;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.3em;">生成一个有趣的引用计数分布，即大多数的专利仅被引用一次，而少部分被引用上百次。</span>
        <span style="display:block;margin-bottom:.3em;">
            编写 MapReduce 程序的第一步是了解数据流。在这个例子中，当 mapper 读取一个记录时它忽略专利号并输出一个键/值对 <citation_count, 1> 作为中间结果。<br />
            Reducer 将所有的 1 加起来算出每种引用次数的和，并将这个和输出。
        </span>
        <span style="display:block;margin-bottom:.3em;">
            基于对数据流的理解, 我们首先为<b>输入(K1、V1)、中间结果 (K2、V2)、输出 (K3、V3)</b> 设定类型。<br />
            这里使用 KeyValueTextInputFormat，它自动将每个输入记录根据分隔符划分为键/值对，并将 K1、V1 设置为 Text 文本类型。<br />
            我们选择 K2、V2、K3、V3 的类型为 IntWritable 类型。
        </span>
        ]]>
    </c:sourceContent>
    
    <c:sourceContent type="" title="CitationHistogram.java (计算不同引用次数专利的数目)" id="sourceContent1" >
      <![CDATA[
public class CititionHistogram extends Configured implements Tool {
    public static class MapClass extends MapReduceBase 
                                 implements Mapper<Text, Text, IntWritable, IntWritable> {
        
        // 出于对效率的考虑，citationCount 和 uno 被定义在类中而不是在方法中定义   
        // 因为减少在 map() 方法中生成对象的个数可以提高性能，并减少垃圾回收
        private final static IntWritable uno = new IntWritable(1);
        private IntWritable citationCount = new IntWritable(1);
        
        // 有多少记录，map 方法就会被调用多少次, 对于每个 JVM 而言就是一个分片中的记录数
        public void map (Text key, Text value, 
                         OutputCollector<IntWritable, IntWritable> output,
                         Reporter reporter) throws IOException {
            citationCount.set(Integer.parstInt(value.toString());
            
            // 按照约定不可以再 output.collect() 方法中修改 citationCount, uno 的值
            output.collect(citationCount, uno);
        }
    }
    
    public static class Reduce extends MapReduceBase 
                               implements Reduce<IntWritable, IntWritable, IntWritable, IntWritable> {
        public void reduce (IntWritable key, Iterator<IntWritable> values, 
                            OutputCollector<IntWritable, IntWritable> output,
                            Reporter reporter) throws IOException {
            int count = 0;
            while (values.hasNext()) {
                count += values.next().get();
            }
            
            output.collect(key, new IntWritable(count));
        }
    }
    
    public int run (String[] args) throws IOException {
        Configuration conf = getConf();
        
        JobConf job = new JobConf(conf, CitationHistogram.class);
        
        Path in = new Path (args[0]);
        Path out = new Path (args[1]);
        
        FileInputFormat.setInputPaths (job, in);
        FileOutputFormat.setOutputPath (job, out);
        
        job.setJobName ("CitationHistogram");
        job.setMapperClass (MapClass.class);
        job.setReducerClass (Reduce.class);
        
        job.setInputFormat (KeyValueTextInputFormat.class);
        job.setOutputFormat (TextOutputFormat.class);
        
        job.setOutputKeyClass (IntWritable.class);
        job.setOutputValueClass (IntWritable.class);
        
        JobClient.runJob (job);
        
        return 0;
    }
    
    public static void main (String[] args) throws IOException {
        int res = ToolRunner.run (new Configuration(), 
                                  new CitationHistogram(),
                                  args);
                                  
        System.exit (res);
    }
}
        ]]>
    </c:sourceContent>
    
    <c:sourceContent type="html" title="" id="sourceContent1" 
                     style="border-top:none;margin-top:1em;background-color:white;margin-bottom:2em;"
                     bodyStyle="background-color:white;padding:0;">
      <![CDATA[
        <span style="display:block;margin-bottom:.5em;">该程序的输出如下: </span>
        <span style="display:block;margin-bottom:.5em;">
              1 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            921128 <br />
              2 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            552246 <br />
              3 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            380319 <br />
              4 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            278438 <br />
              5 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            210814 <br />
              6 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            163149 <br />
              7 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            127941 <br />
              .............<br />
              411 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;          1<br />
              779 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;          1<br />
        </span><br /><br />
        <span style="display:block;margin-bottom:.5em;">
            将以上输出放在图表中并绘制成图，该图形采用双对数坐标系统。
            在双对数图中，当分布为直线时，可以被认为呈指数分布。<br />
            引用计数的直方图似乎与此相符，不过其曲率接近于抛物线，也可以被认为呈对数正态分布。<br />
        </span>
        <img src="/images/citationhistogram.jpg" style="max-width:100%;" />
        <br />
        ]]>
    </c:sourceContent>

    
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  
  <c:entry style="margin-top:2em;">
    <c:title>Hadoop Streaming </c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[Hadoop 也支持用其他语言来编程，这需要用到名为 Streaming 的通用 API。<br />
        在实际应用中，Streaming 主要用作编写简单、短小的 MapReduce 程序，他们可以通过脚本语言编写，开发更加快捷，并能充分利用非 Java 库。]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="html" title="" id="sourceContent1">
      <![CDATA[。。
        ]]>
    </c:sourceContent>
    
      <c:sourceContent type="html" title="使用 combiner 提升性能" id="sourceContent1" style="border-top:none;margin-top:2em;">
      <![CDATA[
      为了使 combiner 可行， driver 必须为 JobConf 对象指定 combiner 类. 可以使用 setCombinerClass 来实现:<br />
      job.setMapperClass (MapClass.class); <br />
      job.setCombinerClass (Combine.class); <br />
      job.setReducerClass (Reduce.class); <br />
      
    <br />
    Combiner 未必会提高性能，可以监控作业的行为来判断是否由 combiner 输出的记录数明显地少于输入记录数。
        ]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>
  
  <c:entry style="margin-top:2em;">
    <c:title>高阶 MapReduce </c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[链接多个 MapReduce <br />
        执行多个数据集的联结 <br />
        生成 Bloom filter]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="html" title="安装 AWS" id="sourceContent1">
      <![CDATA[。。
        ]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

<c:entry style="margin-top:2em;">
    <c:title>划分为多个输出文件 </c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[MapReduce 作业默认将所有输出写到一个单一的文件中。<br />
        MultipleOutputFormat 提供了一个简单的方法，将相似的记录结组为不同的数据集。在写出每条记录之前，这个 OutputFormat 类调用一个内部方法来确定要写入的文件名。<br />
        ]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="" title="MultiFile.java" id="sourceContent1">
      <![CDATA[public class MultiFile extends Configured implements Tool {
    .....  
}]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>


  <c:entry style="margin-top:2em;">
    <c:title>在云上运行 Hadoop </c:title>
    <c:desc>
      <c:desc1>
        <![CDATA[]]>
      </c:desc1>
    </c:desc>

    <c:sourceContent type="html" title="安装 AWS" id="sourceContent1">
      <![CDATA[。。
        ]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>
</c:component>
