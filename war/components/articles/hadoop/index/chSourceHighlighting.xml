<?xml version="1.0" encoding="utf-8"?>
<c:component type="chSourceHighlighting" componentId="chSourceHighlighting_1" xmlns:c="http://com.snnmo.website">
  <c:entry>
    <c:title></c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
  <c:sourceContent type="html" title="分布式系统和 Hadoop" id="sourceContent1">
    <![CDATA[
    <p style="margin-bottom:1em;">对于一个有 4 个 I/O 通道的高端机，即使每个通道的吞吐量各为 100MB/s，读取 4TB 的数据集也需要 3 个小时。
    而利用 Hadoop，同样的数据集会被划分为较小的块 (通常为 64MB)，通过 Hadoop 分布式文件系统 (HDFS) 分布在集群内多台机器上。
    使用适度的复制，集群可以并行读取数据，进而提供很高的吞吐量。这样一组通用机器比一台高端服务器更加便宜。</p>
    <br />
    <p>Hadoop 强调把代码向数据迁移，即： 让数据不动，而将可执行代码发送到数据所在的机器上去。
    数据被拆分后在集群中分布，并且尽可能让一段数据的计算发生在同一台机器上，既这段数据驻留的地方。
    这里所说的代码指的就是 MapReduce 程序。</p>
    ]]>
  </c:sourceContent>
  <c:sourceContent type="html" title="比较 SQL 数据库和 Hadoop" id="sourceContent1" style="margin-top:1em;">
    <![CDATA[
    <p><span style="font-weight:bold">用横向扩展替代纵向扩展：</span>前者加硬件后者加机器</p>
    <p><span style="font-weight:bold">用键值对替代关系表：</span> Hadoop 的数据来源可以使任意形式，但最终会转换为键值对以供处理。</p>
    <p><span style="font-weight:bold">用函数式编程 (MapReduce) 替代声明式查询 (SQL)：</span>对于习惯 SQL 范式的人，用 MapReduce 来思考是一个挑战。</p>
    <p><span style="font-weight:bold">离线处理代替在线处理：</span>Hadoop 适合一次写入、多次读取的数据存储需求。类似于 SQL 世界中的数据仓库。</p>
    ]]>    
  </c:sourceContent>
  

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>扩展一个单词统计程序</c:title>
    <c:desc>
      <c:desc1> <![CDATA[统计一组文档中每个单词出现的次数]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="单词统计" id="sourceContent1" style="border-bottom:none;">
      <![CDATA[
    Do as I say, not as I do.
    ]]></c:sourceContent>


    <c:sourceContent type="" title="" id="sourceContent1" style="border-top:none;border-bottom:none;"><![CDATA[// 实现该程序的伪代码如下;
define wordCount as Multiset;
for each document in documentSet {
  T = tokenize(document);
  for each token in T {
    wordCount[toke]++;
  }
}
display(wordCount);]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;border-bottom:none;">
      <![CDATA[该程序只适合处理少量文档，一旦文档数量激增，它就不能胜任了：<b>使用单台计算机反复遍历所有文档将会非常费时。</b><br /><br />
      重写程序让工作可以分布在多台机器上，每台计算机处理这些文档的不同部分。当所有的机器都完成时，第二个处理阶段将合并这结果。]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-top:none;border-bottom:none;">
      <![CDATA[
// 第一阶段要分布到多台机器上去的伪代码为: 
define wordCount as Multiset;
for each document in documentSet {
  T = tokenize(document);
  for each token in T {
    wordCount[toke]++;
  }
}
sendToSecondPhase(wordCount);

// 第二阶段的伪代码为: 
define totalWordCount as Multiset;
for each wordCount received from firstPhase {
  multisetAdd (totalWordCount, wordCount);
}]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;">
      <![CDATA[为了使该程序工作在一个分布式计算机集群上，需要添加以下功能：<br />
     <p style="font-weight:bold;"> 1、存储文件到许多台计算机上 (第一阶段)。<br />
      2、编写一个基于磁盘的散列表，使得处理不受内存容量限制。 <br />
      3、划分来自第一阶段的中间数据 (即 wordCount)。 <br />
      4、洗牌到这些分区到第二阶段中合适的计算机上。</p>]]>
    </c:sourceContent>

    
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>
  
  <c:entry style="margin-top:2em;">
    <c:title>MapReduce</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <!--c:sourceContent type="html" title="管道和消息队列" id="sourceContent1">
      <![CDATA[
    <p style="margin-bottom:0em;"><b>管道</b>和<b>消息队列</b>等数据处理模型可专用于数据处理应用的方方面面 (Unix pipes 就是一种常见的管道)。</p>
    <p style="margin-bottom:0em;"><b>管道</b>有助于<b>进程原语</b>的重用，已有模块的简单链接即可组成一个新的模块。</p>
    <p style="margin-bottom:0em;"><b>消息队列</b>有助于<b>进程原语</b>的同步，程序员将数据处理任务以生产者或消费者的形式编写为进程原语，由系统来管理它们何时执行。</p>
    
    
    <p style="margin-top:1em;"><b>MapReduce</b> 也是一个数据处理模型，它的最大优点是容易扩展到多个计算节点上处理数据。</p>
    <p style="margin-top:0em;">在 <b>MapReduce</b> 模型中，数据处理原语被称为 mapper 和 reducer。
    分解一个数据处理应用为 mapper 和 reducer 有时是繁琐的，但是一旦以 MapReduce 的形式写好一个应用程序，
    仅需修改配置就可以将它扩展到集群中几百、几千甚至上万台机器上运行。</p>
    ]]>
    </c:sourceContent-->
    <c:sourceContent type="html" title="MapReduce" id="sourceContent1">
      <![CDATA[
    <p><b>MapReduce</b> 算法将查询操作和数据集都分解为组件&mdash;这就是<b>映射 (Map)</b>。
    在查询中被映射的组件可以被同时处理 (即<b>规约:Reduce</b> ) 从而快速地返回结果。不幸的是 <b>MapReduce</b>  是一个在概念和实现上都很复杂的想法！</p>
    <p>MapReduce 算法将查询操作和数据集都分解为组件&mdash;这就是映射 (Map)。
    在查询中被映射的组件可以被同时处理 (即规约:Reduce) 从而快速地返回结果。不幸的是 MapReduce 是一个在概念和实现上都很复杂的想法！</p>
    <br />
    <p><b>管道</b>和<b>消息队列</b>等数据处理模型，用于数据处理应用的方方面面 (Unix pipes 就是一种常见的管道)。
        <b>管道</b>有助于进程原语的重用，已有模块的简单链接即可组成一个新的模块；<b>消息队列</b>则有助于进程原语的同步。
        程序员将数据处理任务编写为进程原语，由系统来管理它们何时执行。</p>
    <br />
    <p>MapReduce 做为一个数据处理模型，它的最大优点是容易扩展到多个计算节点上处理数据。
      在 <b>MapReduce</b> 模型中，数据处理原语被称为 <b>mapper</b> 和 <b>reducer</b>。分解一个数据处理应用为 mapper 和 reducer 有时是繁琐的。
      但是一旦以 MapReduce 的形式写好了一个应用程序，仅需修改配置就可以将它扩展到集群上运行。</p>
    <br />
    <p>MapReduce 程序的执行分为两个主要阶段，即：mapping 和 reducing。每个阶段均定义成一个数据处理函数，分别称为 mapper 和 reducer。
       <b>在 mapping 阶段，MapReduce 获取输入数据并将数据单元装入 mapper。在 reducing 阶段，reducer 处理来自 mapper 的所有输出，并给出最终结果。</b>
       <b style="color:green;">mapper 意味着将输入进行过滤转换 (通常是转换成键值对)，使 reducer 可以完成聚合。</b></p>
    <br />
    <p>MapReduce 使用<b>列表</b>和<b>键/值</b>对作为其主要的数据原语。</p>
    <table style="margin-top:.5em;">
      <thead>
        <tr>
          <th style="padding:.2em .3em;border-right:solid 1px black;"></th>
          <th style="padding:.2em .3em;font-weight:bold;border-right:solid 1px black;border-top:solid 1px black;background-color:rgb(199, 195, 195);">输入</th>
          <th style="padding:.2em .3em;font-weight:bold;border-top:solid 1px black;background-color:rgb(199, 195, 195);">输出</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align:right;padding:.2em .3em;border-right:solid 1px black;border-top:solid 1px black;">map</td>
          <td style="padding:.2em .3em;border-right:solid 1px black;border-top:solid 1px black;">&lt;k1, v1&gt;</td>
          <td style="padding:.2em .3em;border-top:solid 1px black;">list(&lt;k2, v2&gt;)</td>
        </tr>
        <tr>
          <td style="text-align:right;padding:0 .3em;border-right:solid 1px black;border-top:solid 1px black;border-bottom:solid 1px black;">reduce</td>
          <td style="padding:.2em .3em;border-right:solid 1px black;border-top:solid 1px black;border-bottom:solid 1px black;">&lt;k2, list(v2)&gt;</td>
          <td style="padding:.2em .3em;border-top:solid 1px black;border-bottom:solid 1px black;">list(&lt;k3, v3&gt;)</td>
        </tr>
      </tbody>
    </table>
    <br />
    <p><b>在 MapReduce 中编写程序就是定制化 mapper 和 reducer 的过程，其完整的数据流如下：</b></p>
    <ul style="list-style:decimal;margin-left:1.3em;margin-top:.5em;">
      <li>应用的输入必须组织为一个键/值对列表 <b>list(&lt;k1, v1&gt;)</b>。用于处理文件的输入格式通常为 <b>list(&lt;fileName, fileContent&gt;)</b>。
         用于处理类似日志文件的大文件的输入格式为 <b>list&lt;lineNumber, lineContent&gt;</b>。<br /><br /></li>
      <li>含有键值对的列表被拆分，进而通过调用 mapper 的 map 函数对每个单独的键值对 <b>&lt;k1, v1&gt;</b> 进行处理。
      mapper 转换每个 <b>&lt;k1, v1&gt;</b> 对并将之放入 <b>&lt;k2, v2&gt;</b> 对的列表中。
      <br /> <br />
      对于下面的单词统计程序，<b>&lt;String fileName, String fileContent&gt;</b> 被输入 mapper，而其中的 fileName 可以被忽略。
      mapper 可以输出一个 <b>&lt;String word, Integer count&gt;</b> 的列表。<br /><br />
      </li>
      <li>所有 mapper 的输出被聚合到一个包含 <b>&lt;k2, v2&gt;</b> 的巨大列表中。
          所有共享相同 k2 的对被组织在一起形成一个新的键值对 <b>&lt;k2, list(v2)&gt; </b>。
          
          框架让 reducer 来分别处理没一个被聚合起来的 <b>&lt;k2, list(v2)&gt; </b>。
          <br /> <br />
      对于下面的单词统计程序，一个文档的 map 输出的列表中可能出现三次 <b>&lt;'foo', 1&gt;</b>，而另一个文档的 map 输出可能出现两次 <b>&lt;'foo', 1&gt;</b>。
      reducer 看到的聚合对为 <b>&lt;'foo', list(1,1,1,1,1)&gt;</b>，此时 reducer 的输出为 <b>&lt;'foo', 5&gt;</b>。
          <br /> <br />
          每一个 reducer 负责不同的单词，MapReduce 框架自动搜集所有的 <b>&lt;k3, v3&gt;</b> 对，并将之写入文件。
      
      <br /><br /></li>
    </ul>
    ]]>
    </c:sourceContent>
    
    

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>动手扩展一个单词统计程序</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="" title="Do as I say, not as I do." id="sourceContent1"><![CDATA[define wordCount as Multiset;
for each document in documentSet {
  T = tokenize (document);
  for each token in T {
    wordCount[token]++;
  }
}
display(wordCount);]]></c:sourceContent>
    
    <c:comment>
        <c:comment1>
            <![CDATA[]]>
        </c:comment1>
    </c:comment>
  </c:entry>
  
  <c:entry style="margin-top:2em;">
    <c:title>基于 MapReduce 重写单词统计程序</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="" title="单词统计中 map 和 reduce 函数的伪代码" id="sourceContent1"><![CDATA[
/* 因为 map 和 reduce 的输出都是列表，
   所以可以使用 Hadoop 中的 emit 函数生成列表中的元素 */ 
map (String fileName, String document) {
  List<String> T = tokenize (document);
      
  for each token in T {
    emit ((String) token, (Integer) 1);
  }
}
    
reduce (String token, List<Integer> values) {
  Integer sum = 0;      
  for each value in values {
    sum = sum + value;
  }      
  emit ((String) token, (Integer) sum);
}]]></c:sourceContent>



    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>用 Hadoop 统计单词</c:title>
    <c:desc>
      <c:desc1> <![CDATA[从网站 <a href="http://hadoop.apache.org/core/releases.html">http://hadoop.apache.org/core/releases.html</a> 上下载最新的稳定版本]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="在 Mac 上安装 Hadoop" id="sourceContent1">
      <![CDATA[打开 Hadoop 发布包 &rarr; 编辑脚本 conf/hadoop-env.sh &rarr; 将 JAVA_HOME 设置为 Java 安装目录：<br />
      <b>export JAVA_HOME=/Library/Java/Home</b> <br /><br />
      
      不加任何参数的运行 <b>bin/hadoop</b>，显示 Hadoop 的用法文档: <br />
      <img style="margin-top:.5em;width:100%;" src="//c1.staticflickr.com/3/2933/13997007728_8bdab253b0_c.jpg" />
      
      <br />
      <br />
      运行 hadoop 示例程序：<b>bin/hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.2.0.jar wordcount</b> 
      <br />
      <br />
      将提示 wordcount 程序的参数列表：<br />
      <b>wordcount [-m &lt;maps&gt;] [-r &lt;reduces&gt;] &lt;input&gt; &lt;output&gt;</b><br /><br />
      
      <b>&lt;input&gt;</b> 指的是所需分析的文本文档的输入目录。(可以从<a href="http://www.gpoaccess.gov/sou/">这里</a>下载所需的文本文件)<br />
      <b>&lt;output&gt;</b> 指的是程序填充结果的输出目录。<br /><br />
      
      程序运行完毕之后，在 output 文件夹下将看到所有单词的统计结果...
]]>
    </c:sourceContent>



    <c:comment>
      <c:comment1>
        <![CDATA[Windows 也可以支持开发模式，但你需要在节点上安装 <a href="http://www-cygwin.com/">cygwin</a> 来支持 shell 和 Unix 脚本]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>修改 wordcount 源码</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="" title="建立 WordCount.java 源码副本" id="sourceContent1"><![CDATA[mkdir playground
mkdir playground/src
mkdir playground/classes
cp hadoop-2.2.0-src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/WordCount.java
        playground/src/WordCount.java

// 在 Hadoop 框架中编译和执行该副本
javac -classpath hadoop-*-core.jar -d playground/classes
        playground/src/WordCount.java
        
jar -cvf playground/wordcount.jar -C playground/classes/ .

// 运行该副本 (如 output 目录存在应先将其删除)
bin/hadoop jar playground/wordcount.jar org.apache.hadoop.examples.WordCount input output]]></c:sourceContent>

    <c:sourceContent type="" title="修改 WordCount.java 源码" id="sourceContent1"><![CDATA[package org.apache.hadoop.examples;

import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.GenericOptionsParser;

public class WordCount {

  public static class TokenizerMapper 
       extends Mapper<Object, Text, Text, IntWritable>{
    
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
      
    public void map(Object key, Text value, Context context) 
                                  throws IOException, InterruptedException {
      // 使用空格进行分词 
      //StringTokenizer itr = new StringTokenizer(value.toString());                           
      
      // 根据标点符号分词 
      StringTokenizer itr = new StringTokenizer(value.toString(), " \t\n\r\f,.:;?![]'");   
      
      while (itr.hasMoreTokens()) {      
        // 在 Hadoop 中, 特殊的 Text 类取代了 String。
        // 把 Token 放入 Text 对象中
        // 在将 String 转成 Text 之前，先将所有单词转成小写.
        // word.set(itr.nextToken());              
        word.set(itr.nextToken().toLowerCase());  
                                                  
        context.write(word, one);
      }
    }
  }
  
  public static class IntSumReducer 
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) 
                                  throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      
      // 仅统计数量大于 4 的单词
      // 输出每个 Token 的统计结果
      if (sum > 4) context.write(key, result);           
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();
    if (otherArgs.length != 2) {
      System.err.println("Usage: wordcount <in> <out>");
      System.exit(2);
    }
    Job job = new Job(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}]]></c:sourceContent>

    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>
  
  <c:entry style="margin-top:2em;">
    <c:title>Hadoop 的构造模块</c:title>
    <c:desc>
      <c:desc1> <![CDATA[在一个全配置的集群上，运行 Hadoop 意味着在网络分布的不同服务器上运行一组守护进程。
      这些守护进程有特殊的角色，一些仅存在于单个服务器上，一些运行在多个服务器上。]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="Hadoop 的守护进程" id="sourceContent1">
      <![CDATA[<p style="font-weight:bold;">
      NameNode (名字节点) <br />
      DataNode (数据节点) <br />
      Secondary NameNode (次名字节点) <br />
      JobTracker (作业跟踪节点) <br />
      TaskTracker (任务跟踪节点)</p> ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="NameNode" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>Hadoop 在<b>分布式计算</b>与<b>分布式存储 (HDFS)</b>中均采用了<b>主/从 (master/slave)</b>结构。<br />
      <b>2、</b>NameNode 位于 HDFS 的主端，它指导从端的 DataNode 执行底层的 I/O 任务。<br />
      <b>3、</b>NameNode 是 HDFS 的书记员，它跟踪文件如何被分割成文件块，这些块又被哪些节点存储，以及分布式文件系统的整体运行状态是否正常。<br />
      <b>4、</b>运行 NameNode 会消耗大量内存和 I/O 资源，所以驻留 NameNode 的服务器通常不会存储用户数据或者执行 MapReduce 程序的计算任务。<br />
      <b>5、</b>除了 NameNode，其它任何守护进程驻留的节点发生故障或失效均不会影响 Hadoop 集群的整体运行。]]>
    </c:sourceContent>

    <c:sourceContent type="html" expand="false" title="DataNode" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>Hadoop 集群上的所有节点都会驻留一个 DataNode 守护进程，来执行<b>HDFS</b>的繁重工作 (将 HDFS 数据块读取或写入到本地文件系统的实际文件中)。<br />
      <b>2、</b>当希望对 HDFS 文件进行读写时，文件被分割为多个块，由 NameNode 告知客户端每个数据块驻留在哪个 DataNode 节点。<br />
      <b>3、</b>客户端直接与 DataNode 守护进程通信，来处理与数据块相对应的本地文件。而后，DataNode 会与其它 DataNode 节点通信，复制这些数据块以实现冗余。<br />
      <b>4、</b>下图是 NameNode 和 DataNode 在 HDFS 中的交互。NameNode 跟踪文件的元数据--描述系统中所包含的文件以及每个文件如何被分割为数据块。
      DataNode 提供数据块的备份存储，并持续不断的向 NameNode 报告，以保持元数据为最新状态。<br />
      <img src="//c1.staticflickr.com/3/2939/14227977545_b73c8e45ca_z.jpg" style="width:100%;" /><br />
      <b style="color:green;">在上图中有两个数据文件，一个位于目录 /user/chuck/data1，另一个位于 /user/james/data2。
      data1 有 1、2、3 三个数据块，data2 有 4、5 两个数据块。
      其中每个数据块有三个副本，这确保了任何一个 DataNode 崩溃，数据文件仍然可以访问。
      DataNode 会不断地更新 NameNode，为之提供本地修改的相关信息，同时接收相关指令以进行创建、移动、或删除本地磁盘上的数据块。</b>]]>
    </c:sourceContent>

    <c:sourceContent type="html" expand="false" title="Secondary NameNode (SNN)" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>SNN 是一个用于监测 HDFS 集群状态的辅助守护进程。它与 NameNode 通信，根据集群所配置的时间间隔获取 HDFS 元数据的快照。<br />
      <b>2、</b>NameNode 是 Hadoop 集群的单一故障点，而 SNN 的数据快照有助于减少停机的时间并降低数据丢失的风险。<br />
      <b>3、</b>NameNode 的失效处理需要人工的干预，即手动地重新配置集群，将 SNN 用作主要的 NameNode。<br />
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" expand="false" title="JobTracker" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>JobTracker 守护进程是应用程序和 Hadoop 之间的纽带。
      一旦提交代码到集群上，JobTracker 就会确定执行计划，包括决定处理哪些文件、为不同的任务分配节点以及监控所有任务的运行。<br />
      <b>2、</b>如果任务失败，JobTracker 将自动重启任务，同时受到预定义的重试次数限制。<br />
      <b>3、</b>每个 Hadoop 集群只有一个 JobTracker 守护进程，它通常运行在服务器集群的主节点上。
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" expand="false" title="TaskTracker" id="sourceContent1" style="margin-top:1em;">
      <![CDATA[
      <b>1、</b>与存储的守护进程一样，计算的守护进程也遵循主/从架构： JobTracker 作为主节点，监测 MapReduce 作业的整个执行过程。<br />
      <b>2、</b>TaskTracker 管理各个任务在每个从节点上的执行情况，并负责执行由 JobTracker 分配的单项任务。<br />
      <b>3、</b>每个从节点上仅有一个 TaskTracker，但每个 TaskTracker 可以生成多个 JVM 来并行地处理许多 map 或 reduce 任务。<br />
      <b>3、</b>TaskTracker 的一个职责是持续不断地与 JobTracker 通信。
            如果 JobTracker 在指定的时间内没有收到来自 TaskTracker 的 "心跳"，它会假定 TaskTracker 已经崩溃了，进而重新提交相应的任务到集群中的其它节点中。<br />
            <br />
      <b>Hadoop 集群的拓扑结构：</b><br />
      <img src="//c1.staticflickr.com/3/2915/14248462943_c30a94a4af.jpg" style="width:100%;" /><br />
      <b>这个拓扑结构遵从主/从架构，其中 NameNode 和 JobTracker 为主端，DataNode 和 TaskTracker 为从端。</b>
      ]]>
    </c:sourceContent>


    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  <c:entry style="margin-top:2em;">
    <c:title>组建一个完整的 Hadoop 集群</c:title>
    <c:desc>
      <c:desc1> <![CDATA[1、首先建立主节点以及节点之间的控制通道，即 Secure Shell (SSH) 通道。]]></c:desc1>
      <c:desc1> <![CDATA[2、SSH 采用标准的公钥加密来生成一对用户验证密钥--一个公钥、一个私钥。
      公钥存储在集群的每个节点上，私钥则由主节点在试图访问远端节点时发送过来。结合公钥与私钥，目标机可以对这次访问进行验证。]]></c:desc1>
      <c:desc1> <![CDATA[3、所有节点的登陆账号应该有相同的用户名 (例如使用：hadoop-user)]]></c:desc1>
      <c:desc1> <![CDATA[4、其次建立单机或者伪分布模式或者标准的集群安装模式 (全分布模式)]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="验证 SSH 安装" id="sourceContent1">
      <![CDATA[<b>$which ssh<br />
      $which sshd<br />
      $which ssh-keygen </b><br />
      
      <br />
      如果收到如下消息，说明没有安装：<br />
      <b>$no ssh in (/user/bin:/bin:/user/sbin .... </b><br />
      <br />
      可以通过 Linux 安装包管理器安装 OpenSSH (www.openssh.com) 或者直接下载其源代码。
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="生成 SSH 密钥对" id="sourceContent1">
      <![CDATA[<b>使用主节点上的 <b>ssh-keygen</b> 生成一个 RSA 密钥对 (注意避免输入口令)。</b><br />
      <img style="max-width:100%;" src="//c1.staticflickr.com/3/2923/14086005918_0e28bff367_b.jpg" />
      <br />
      <img style="max-width:100%;" src="//c2.staticflickr.com/6/5525/14272256944_1659236551_b.jpg" />
      ]]>
    </c:sourceContent>
    
    <c:sourceContent type="html" title="Hadoop 配置目录" id="sourceContent1">
      <![CDATA[
      <img style="max-width:100%;" src="//c2.staticflickr.com/6/5579/14281021394_238533831e_b.jpg" /><br />
      <br />
      指定包括主节点在内的所有节点上的 jdk 安装路径 (即在 <b>hadoop-env.sh</b> 中定义 JAVA_HOME 环境变量使之指向 Java 安装目录)
      <br />
      <b>export JAVA_HOME=/user/share/jdk</b><br />
      <br />
      <b>1、</b>0.20 版本以前： Hadoop 的设置主要包含在 <b>hadoop-default.xml</b> 和 <b>hadoop-site.xml</b> 文件中。 <br />
      <b>2、</b>hadoop-default.xml 中包含了 Hadoop 会使用的默认设置，除非这些设置在 hadoop-site.xml 中被显示地重新定义。
      因此实际操作中只需要处理 hadoop-site.xml。<br />
      <b>3、</b>在 0.20 版本中：hadoop-site.xml 文件被拆分成 3 个 xml 文件。
      分别是：<b>core-site.xml、hdfs-site.xml、mapred-site.xml</b>。
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="单机模式" id="sourceContent1">
      <![CDATA[
     这种模式下所有 3 个 xml 文件均为空。当配置文件为空时，Hadoop 将完全运行在本地。
      ]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="伪分布模式" id="sourceContent1" style="border-bottom:none;">
      <![CDATA[
     该模式增加了代码调试功能，允许程序员检查内存使用情况、HDFS 输入输出、以及其它的守护进程交互。<br />
     <br />
     <b>伪分布模式下 3 个 xml 文件示例：</b><br /><br /><br /><br />
     <i><b>core-site.xml</b></i>
      ]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;"><![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>fs.default.name</name>
          <value>hdfs://localhost:9000</value>
          <description>
              The name of the default file system. 
              A URI whose scheme and authority determine the 
              FileSystem implementation.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<br /><i><b>mapred-site.xml</b></i>]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;"><![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>mapred.job.tracker</name>
          <value>localhost:9001</value>
          <description>
              The host and port that the MapReduce job tracker runs at.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>

    <c:sourceContent type="html" title="" id="sourceContent1" style="border-bottom:none;border-top:none;">
      <![CDATA[<br /><i><b>hdfs-site.xml</b></i>]]>
    </c:sourceContent>
    <c:sourceContent type="" title="" id="sourceContent1" style="border-bottom:none;border-top:none;"><![CDATA[<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
      <property>
          <name>dfs.replication</name>
          <value>1</value>
          <description>
              The actual number of replications can be specified when the file is created.
          </description>
      </property>
</configuration>]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="" id="sourceContent1" style="border-top:none;">
      <![CDATA[<b style="color:red;">以上 3 个文件的配置中分别在 core-site.xml 和 mapred-site.xml 中指定了 NameNode 和 JobTracker 的主机名与端口。
      在 hdfs-site.xml 中指定了 HDFS 的默认副本数。</b>
      <br />
      <br />
      接下来需要在 masters 文件中指定 SNN 的位置，并在 slaves 中指定从节点的位置： <br />
      <b>cat masters<br />
      localhost
      <br />
      cat slaves <br />
      localhost</b>
      <br />
      <br />
      接下来检查机器是否允许对自己运行 ssh：<br />
      <b>ssh localhost</b>
      <br />
      如果不允许，需要用两行命令来安装：<br />
      <b>ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa <br />
      cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys </b>
       <br />
       <br />
      接下来输入一个命令格式化 HDFS：<br />
      <b>bin/hadoop namenode -format</b>
      <br />
      <br />
      接下来使用 start-all.sh 脚本装在守护进程，然后用 Java 的 jps 命令列出所有守护进程来验证安装是否成功：<br />
      <b>bin/start-all.sh<br />
      jps</b>
      <br />
      <br />
      <b style="color:gray;">26893 Jps<br />
      26832 TaskTracker<br />
      26620 SecondaryNameNode<br />
      26333 NameNode<br />
      26484 DataNode<br />
      26703 JobTracker</b><br />
      ]]>
    </c:sourceContent>
    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>

  

  <c:entry style="margin-top:2em;">
    <c:title>Hadoop 组件</c:title>
    <c:desc>
      <c:desc1> <![CDATA[]]></c:desc1>
    </c:desc>
    <c:sourceContent type="html" title="HDFS" id="sourceContent1">
      <![CDATA[HDFS 是一种分布式文件系统，专为 MapReduce 这类框架下的大规模分布式数据处理而设计的。可以把一个 100TB 的大数据集在 Hadoop 中存储为单个文件。<br />
      Hadoop 提供了一套与 Linux 文件命令类似的命令行工具，用于与 HDFS 交互。
      ]]>
    </c:sourceContent>
    <c:sourceContent type="html" title="HDFS 文件操作" id="sourceContent1">
      <![CDATA[<b>假设已经完成了 HDFS 格式化，并启动了一个 HDFS 文件系统 (可以使用伪分布模式)。</b>
      <br />  <br />
      
      <b style="color:green;">基本文件命令：</b>
      <br />
      <b>hadoop fs -cmd &lt;args&gt;</b><br /><br />
      <b>hadoop fs -ls &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 文件列表</b>
      <br />  
      <b>将会产生如下输出：</b><br />
      Fount N items <br />
      -rw-r--r-- <b>1</b> keesh supergroup 264 2014-05-26 18:17 /user/keesh/filename.txt<br />
      其中 1 代表复制因子 (伪分布式下永远为 1，集群环境通常为 3。)
      <br />  <br />
      
      <b style="color:green;">添加文件和目录：</b>
      <br />
      <b>HDFS 有一个默认的工作目录 /user/$USER，($USER 是登陆名，并且这个目录不会自动建立，可以 mkdir 创建它。)</b><br /><br />
      <b>hadoop fs -mkdir /user/keesh &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 创建目录 (将自动创建父级目录)</b><br />
      <b>hadoop fs -lsr /  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 查看所有文件和子目录</b><br />
      <b>hadoop fs -put example.txt .  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 将 example.txt 复制到 Hadoop 的默认工作目录 (. 代表默认目录)</b><br />
      <b>hadoop fs -put example.txt /user/keesh  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 等价于上面的命令</b><br />
      <b>hadoop fs -get example.txt .  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 将 example.txt 文件从 Hadoop 中复制到本地当前目录</b><br />
      <b>hadoop fs -cat example.txt   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 显示 example.txt 文件的内容</b><br />
      <b>hadoop fs -rm example.txt   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 删除文件或目录</b><br />
      ]]>
    </c:sourceContent>



    <c:comment>
      <c:comment1>
        <![CDATA[]]>
      </c:comment1>
    </c:comment>
  </c:entry>
</c:component>
